% Created 2020-12-27 Sun 23:59
% Intended LaTeX compiler: pdflatex
\documentclass[sigconf,natbib,screen=true,review=true,anonymous]{acmart}

% We'll get the submission number fro the submission system
\acmSubmissionID{2043}

\input{packages}
\input{definitions}
\input{authors}
\input{meta}

\begin{document}


\title[sigmoidF1: A Smooth F1 Score Surrogate Loss for Multilabel Classification]{sigmoidF1: A Smooth F1 Score Surrogate Loss for Multilabel Classification}
  % SIMPUL: A Loss Framework to Learn Abstract Labels from Abstract Representations: Single-Instance Multiclass Multilabel Prediction with Unknown Label Count}

\begin{abstract}

  Multiclass multilabel classification refers to the task of attributing multiple labels to examples via predictions. Current models formulate a reduction of that multilabel setting into either multiple binary classifications or multiclass classification, allowing for the use of existing loss functions (sigmoid, cross-entropy, logistic, etc.). Empirically, these methods have been reported to achieve good performance on different metrics (F1 score, Recall, Precision, etc.). Theoretically though, the multiclass classification reduction does not allow one to predict varying numbers of labels per example, because the underlying losses are distant estimates of the performance metrics.\daan{This suggests a theoretical contribution.}

  We propose a loss function, sigmoidF1. It is an approximation of the F1 score that (I) is smooth and tractable for stochastic gradient descent, (II) naturally approximates a multilabel metric, (III) estimates label propensities and label counts. More generally, we show that any confusion matrix metric can be formulated with a smooth surrogate. We evaluate the proposed loss function on different text and image datasets, and with a variety of metrics, to account for the complexity of multilabel classification evaluation. In our experiments, we embed the sigmoidF1 loss in a classification head that is attached to state-of-the-art efficient pretrained neural networks MobileNetV2 and DistilBERT.

Our experiments show that sigmoidF1 outperforms other loss functions on four datasets and several metrics. These results show the effectiveness of using inference-time metrics as loss function at training time in general and their potential on non-trivial classification problems like multilabel classification.

\if0
its smooth -> section 3.1 & figure 1
naturally approximate the multilabel metric of choice -> experimental results, table 3

(label count and label prediction)

maybe remove the correlation claim YES

add that we give the first formalization of unboundedF1

instead of reformulating the estimand entirely, we just allow it to be stochastic instead of deterministic.

by definition reg(f_n)


people usually implement this methods with cross-entropy, we flollow in their footsteps.

leave multiclass classification

Note that for many real world problems and datasets reducing the problem to top-k selection or establishing a hierarchical structure is an oversimplification, especially when classes are not mutually exclusive. Besides the four datasets we use for our experiments, we mention others in the related work section.

\fi


% \mdr{Multilabel classification is the task of classifying XXX.}
% \mdr{In multilabel classification with unknown label counts we do not know XXX.}
% \mdr{Multilabel classification with unknown label counts are very common in IR.}
% \mdr{Current approaches to Multilabel classification with unknown label counts are characterized by XXX and suffer from YYY.}
% \mdr{We propose XXX to address Multilabel classification with unknown label counts.}
% \mdr{We evaluate our proposal on XXX.}
% \mdr{We find XXX.}
% \mdr{Broader implications.}
\if0
Multilabel classification is a common task when learning from text, image, or sound. However, few optimization frameworks are tailored towards learning multiple abstract labels for an entire text, image, or sound. State of the art models for image and text, of the CNN and BERT family respectively, are often benchmarked on multiclass multilabel datasets, but still use loss functions adapted to a multiclass unilabel situation (i.e. variations of cross-entropy losses).

When the number of ground-truth labels varies over each example as is commonly the case in multilabel classification, these losses produce unit-interval results that require a sophisticated thresholding regime (at training or at inference time) to predict both label prediction propensity and label count.

Since certain confusion matrix metrics usually already fulfill that goal at inference time, we propose decomposable surrogates for gradient descent at training time. We illustrate the solution with \emph{sigmoidF1}: a decomposable surrogate F1 score that introduces smooth thresholding. We are able to demonstrate its performance on efficient versions of the state of the art models (DistilBert and MobileNetV2). For a fair assessment of the proposed loss function, we avoid datasets that would benefit from a two stage modeling procedure (object / expression / segment detection followed by classification) and instead turn to datasets, where the whole example is predictive of a label.
\fi
\end{abstract}


\maketitle

\acresetall

\setlength{\belowdisplayskip}{0.5\baselineskip} \setlength{\belowdisplayshortskip}{0.5\baselineskip}
\setlength{\abovedisplayskip}{0.5\baselineskip} \setlength{\abovedisplayshortskip}{0.5\baselineskip}

\input{sections/01-introduction}
\input{sections/02-background}
\input{sections/03-method}
\input{sections/04-experimental-setup}
\input{sections/05-experimental-results}
\input{sections/06-related-work}
\input{sections/07-conclusions}

\section*{Reproducibility}
To facilitate the reproducibility of the reported results, this work only made use of publicly available datasets and our experimental implementation is publicly available at [URL withheld for double blind review].

\begin{acks}
 This work was supported by many people.
 All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
