% Created 2020-12-27 Sun 23:59
% Intended LaTeX compiler: pdflatex
\documentclass[sigconf,natbib,screen=true,review=true,anonymous]{acmart}

% We'll get the submission number fro the submission system
\acmSubmissionID{848}

\input{packages}
\input{definitions}
\input{authors}
\input{meta}

\begin{document}

\title[Confusion Matrix Metrics as Losses for Multi-label Classification]{Confusion Matrix Metrics as Losses for \\ Multi-label Classification with Unknown Label Counts}

  % SIMPUL: A Loss Framework to Learn Abstract Labels from Abstract Representations: Single-Instance Multiclass Multilabel Prediction with Unknown Label Count}

\begin{abstract}
  
\mdr{Multilabel classification is the task of classifying XXX.}
\mdr{In multilabel classification with unknown label counts we do not know XXX.}
\mdr{Multilabel classification with unknown label counts are very common in IR.}
\mdr{Current approaches to Multilabel classification with unknown label counts are characterized by XXX and suffer from YYY.}
\mdr{We propose XXX to address Multilabel classification with unknown label counts.}
\mdr{We evaluate our proposal on XXX.}
\mdr{We find XXX.}
\mdr{Broader implications.}
\if0
  Multilabel classification is a common task when learning from text, image, or sound. However, few optimization frameworks are tailored towards learning multiple abstract labels for an entire text, image, or sound. State of the art models for image and text, of the CNN and BERT family respectively, are often benchmarked on multiclass multilabel datasets, but still use loss functions adapted to a multiclass unilabel situation (i.e. variations of cross-entropy losses). When the number of ground-truth labels varies over each example as is commonly the case in multilabel classification, these losses produce unit-interval results that require a sophisticated thresholding regime (at training or at inference time) to predict both label prediction propensity and label count. Since certain confusion matrix metrics usually already fulfill that goal at inference time, we propose decomposable surrogates for gradient descent at training time. We illustrate the solution with \emph{sigmoidF1}: a decomposable surrogate F1 score that introduces smooth thresholding. We are able to demonstrate its performance on efficient versions of the state of the art models (DistilBert and MobileNetV2). For a fair assessment of the proposed loss function, we avoid datasets that would benefit from a two stage modeling procedure (object / expression / segment detection followed by classification) and instead turn to datasets, where the whole example is predictive of a label. 
\fi
\end{abstract}


\maketitle

\acresetall

\input{sections/01-introduction}
\input{sections/02-background}
\input{sections/03-method}
\input{sections/04-experimental-setup}
\input{sections/05-experimental-results}
\input{sections/06-analysis}
\input{sections/07-related-work}
\input{sections/08-conclusions}

\section*{Reproducibility}
To facilitate the reproducibility of the reported results, this work only made use of publicly available data and our experimental implementation is publicly available at \url{https://anonymous.4open.science/something}.

\begin{acks}
 This work was supported by many people.
 All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
