% Created 2020-12-27 Sun 12:35
% Intended LaTeX compiler: pdflatex
\documentclass[sigconf,natbib,screen=true,review=true,anonymous]{acmart}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
% We'll get the submission number fro the submission system
\acmSubmissionID{xx}
\input{packages}
\input{definitions}
\input{authors}
\input{meta}
\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{TODO : #1}}
\newcommand\doubt[1]{\textcolor{orange}{DOUBT : #1}}
% \newcommand\todo[1]{} % uncomment to hide comments
% \newcommand\doubt[1]{} % uncomment to hide comments
\usepackage{dsfont}
\usepackage{color}
\author{Gabriel Bénédict}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={Gabriel Bénédict},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.1 (Org mode 9.1.14)}, 
 pdflang={English}}
\begin{document}

\title{Loss framework for entity-wide multiclass multilabel prediction with varying number of labels}


\begin{abstract}
Multilabel classification is a common task in text, image or video (scene) prediction.
\end{abstract}


\keywords{Keyword; Keyword; Keyword}

\maketitle

\acresetall

\section{introduction}
\label{sec:orgf5776a7}

As neural network models are able to learn increasingly abstracter representations via deeper networks, representation learning and self-supervision, it might be reasonable to expect that, thanks to their conferred broader understanding of the world, they get better at predicting more abstract labels. Beyond objects types, face recognition, expressions, neural networks might be able to predict genres/categories \todo{other things as well?} of text, image and sound. While researchers are working hard at building neural networks with very high level of understanding in the embedding space, there seems to be few research on developing loss functions that are adapted for these higher level concepts in the output space.

Although multilabel binary prediction (commonly referring to mutually inclusive labels) is a task thoroughly covered in existing litterature, there does not seem to exist a framework that deals with different amounts of positive labels in the groundtruth. For example, a scientific journal can be tagged as \emph{machine learning} and \emph{economics}, or a movie can be tagged as \emph{romance} and \emph{comedy}. These instances might as well be assigned only one tag in the groundtruth, or many more within the possible tags (classes).

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./tree/Tree.pdf}
\caption{\label{fig:tree}
"multiclass" nomenclature}
\end{figure}

Before, exploring the subject further, we will use Figure \ref{fig:tree} to disambiguate the terminology used in this research. There seems to exist a concensus over the terms multiclass and multilabel learning, meaning respectively mutually exclusive and mutually inclusive labels \todo{source}. Multilabel can therefore be seen as a subdomain of multiclass learning, where more than one class can be true for the same example. Within multilabel training, we introduce the distinction between sub-entity unilabel and entity-wide multilabel. The former refers to tasks where elements within each example can be singled-out (objects in an image or expressions in a text) and assigned a single label. In the countrary, this paper focusses on entity-wide multilabel training (sparse occurences of the term holistic can be found in the litterature to describe this phenomenon for image \cite{holisticImageDescriptors,holisticLungs} and a recent video dataset \cite{holisticVideoData} \todo{read these}), more specifically with varying label counts. To the best of our knowledge, there are few existing representatives of that type of labelling task in the literature. \todo{cite more milestone examples for each category.} \todo{delta with hierarchical label learning}

The particularity of tasks like scientific paper tagging or movie genre classification is that it remains unclear what elements in an image/video or text can be singled out as predictive of a particular tag/genre. Rather, a complex interaction between these elements in the feature space steer the predictions. For example, the sole mention of the term "machine learning" in a paper should not be a sufficient condition to tag it as such. Instead, one could expect from the publisher to get acquainted with the paper enough to determine wether the research is a worthwhile contribution or application of \emph{machine learning} to deserve the tag. This involves thorough understanding of the proposed method and background knowledge on state-of-the-art methods. An analogous argument can be made for movie genre classification for movie posters.

However, if elements in an image/text can be singled out as predictive of a single tag, the problem reverts back to predicting with the a priori knowledge of the existence of only one true label (i.e. sub-entity unilabel learning).  The reason for distanciating singling-out from entity-wide labels, is that it has been shown that as soon as singling-out is possible, models that work on instances are more accurate \todo{rewrite this paragraph and sources}. The singled-out elements can be subsets of the original feature space (typically in object detection like with the COCO dataset  \cite{COCO} and \todo{others}). Similarly, recent research has shown that the singled-out elements can be located in the abstract representations (embeddings) of the feature set and might individually predict a single true label (like GPT-3 \todo{source}) \todo{more examples}. This might also carry prospects of generalizability of the model \cite{generalization} \todo{elaborate}. 

But for now, in certain retrieval tasks such as scientific journal tagging, the effect of sub-entities (either expressions in the text or single features in the embedding space) on the prediction of each label remains hard to assess. Instead we propose entity-wide (sometimes referred to as holistic) multilabel learning for varying amount of labels, with a focus on custom loss functions.

To allow the use of existing diffentiable loss fonctions, previous research papers tend to reframe the problem into either (I) a sub-entity uni-label (as described above, with the COCO dataset as an example of isolation of features \cite{COCO}), (II) entity-wide unilabel prediction (III) entity-wide multilabel prediction with fixed label count (IV) entity-wide multilabel prediction with varying label count with post-training thresholding (V) redefine backpropagation for multilabel prediction \cite{multilabelBackprop} (VI) multitask learning \cite{multitaskLabel}. This order reflects in ascending order how close modelling seem to fit the original task, which remains entity-wide multilabel learning with varying amounts of labels. \todo{group them}

Sometimes it seems useful to optimize a neural network directly on the evaluation metric \cite{optimizableLosses}. In the case of entity-wide multilabel classification with varying amount of labels, common loss functions such as cross-entropy loss or multinomial logit loss deliver predictions on the unit interval. In the case of mutually exclusive labels, this is a viable solution. Sometimes the groundtruth consists of a collection of positive and negative valued classes, in varying amounts across observations. In other terms, one is  looking for a top-k prediction with varying \(k\) accross observations.



For the problem definitions (III) and (IV)


In a number of retrieval tasks, a model's out of sample accuracy is measured on metrics such as AUROC, F1 score, etc. These reflect an objective catered towards evaluating the model over an entire ranking. Due to to lack of differentiability, these metrics cannot be directly used as loss functions at training time (in-sample). A seminal study \cite{optimizableLosses} derived a general framework for doing so. 

\textbf{\textbf{our contribution}}

We propose a general mathematical formulation of entity-wide multilabel learning for varying amount of groundtruth labels. The generalization encompasses different levels of complexity, from the classical cross-entropy loss up to the proposed loss function. \emph{sigmoidF1} is a F1 score surrogate which allows to optimize for label prediction and count simultanuously in a single task and is robust to outliers. It delivers more precise predictions than the current state-of-the-art on several different metrics, accross text and image related tasks.


\section{Building up on losses}
\label{sec:org2b0f4c8}

Multi-label learning can be divided into two major fields: \emph{problem transformation} and \emph{algorithm adaptation} \cite{multilabelReview}. In the former case, multilabel classification is reframed as a binary, multiclass classification or label ranking problem. In the latter, one tries to adapt multiclass algorithms to the problem. The current endeavour focusses on \emph{algorithm adaptation}.


For the purpose of \emph{problem transformation}, we define \(\mathcal{L}_{\text {multiclass}}\), a class of loss functions that minimize predictions in relative terms. Binary cross-entropy, logit and their variants such as focal loss or hinge loss (deemed unstable \cite{focalLoss}) are common choices when it comes to multiclass prediction. Cross-entropy loss can be formulated as \(\mathcal{L}_{\text {CE}}=-\sum \log \left(p_{i}\right)\) . Note that minimizing binary cross-entropy is equivalent to maximizing for log-likelihood \cite[Section 4.3.4]{Bishop}. More generally, the \emph{problem transformation} formulation amounts to minimizing the loss on a class of neural networks, such that

\begin{equation}
\underset{\mathcal{L}_{\text {multiclass}}} {\min} \mathcal{F}\left(\cdot ; \Theta; \mathcal{L}_{\text {multiclass}} (\mathbf{y}, \hat{\mathbf{y}}) \right),
\end{equation}

In the context of \emph{algorithm adaptation}, where the number of positive labels in the groundtruth is unknown a priori, we aim to both obtain a propensity of each label being true and a prediction of the number of true labels: 

\begin{equation}
\underset{\mathcal{L}_{\text {multiclass}}, \mathcal{L}_{\text {count}}} {\min} \mathcal{F}\left(\cdot ; \Theta; \mathcal{L}_{\text {multiclass}} (\mathbf{y}, \hat{\mathbf{y}}) + \lambda \mathcal{L}_{\text {count}} (\mathbf{n}, \hat{\mathbf{n}})\right),
\end{equation}

where \(n_i = \sum_j \mathds{1}_{\mathbf{y_i^j} = 1}\) is the count of positive labels per example. We thus impose a constraint for the retrieval of label counts. For example, a cross-entropy loss surrogate would penalize for the number of wrongly predicted labels \(\mathcal{L}_{\text {CE+N}}= \mathcal{L}_{\text {CE}} + \lambda (\sum tp / \sum p)\), with \(t p=\sum_{i \in Y^{+}} \mathds{1}_{\mathbf{p_i} \geq b}\) and \(b\) a threshold to be defined. \todo{tencent loss}.

This formulation is most straightfoward but suffers from higher parametrization and the lack of modelling of the interactions between label counts and label prediction. To mitigate these issues, we propose a unified loss formulation, namely

\begin{equation}
\underset{\mathcal{L}_{\text {multitag}}} {\min} \mathcal{F}\left(\cdot ; \Theta; \mathcal{L}_{\text {multitag}} (\mathbf{y}, \hat{\mathbf{y}}, \mathbf{n}, \hat{\mathbf{n}}) \right),
\end{equation}

Although predictions and counts explicitely appear in that formulation, \(\mathcal{L}_{\text {multitag}}\) can optimize for both metrics implicitely (see proposed \emph{sigmoidF1} below).


\todo{look at YOU ONLY TRAIN ONCE: LOSS-CONDITIONAL TRAINING OF DEEP NETWORKS}

\todo{cite stat learning}   \cite[p. 308-310]{statLearning}

\section{related work}
\label{sec:org5bfaf5e}

\todo{look at [[https://www.sciencedirect.com/topics/computer-science/extractive-summarization][extractive summarization]]}

This section will be guided by the previous section's formulation of the multitags problem, we will therefore focus on \emph{algorithm adaptation}, \emph{metrics as losses} and \emph{dynamic thresholding}.

\subsection{algorithm adaptation}
\label{sec:orge9c5683}

Early representatives of \emph{algorithm adaptation} stem from heterogenous domains of machine learning. Multi-Label k-Nearest Neighbors \cite{ML-KNN}, Multi-Label Decision Tree \cite{ML-DT}, Ranking Support Vector Machine \cite{multilabelSVM} and Backpropagation for Multi-Label Learning \cite{multilabelBackprop}. More recently, two papers introduced the idea of multitask learning for \emph{label prediction} and \emph{label count prediction} for text (ML\(_{\text{NET}}\)) \cite{multitaskLabel} and image \cite{multitaskLabelImages} data. The latter research is loosely catered towards object detection (although not formally presented as such) and is thus out-of-scope: elements in a picture are predicted that tend to be unilabel as defined by the groundtruth (e.g. cat, flower, vase, person, bottle etc.).

\subsection{metrics as losses}
\label{sec:org6c111d9}

Often, machine learning post-training evaluation metrics (e.g. AUROC, F1) are not differentiable. There are motivations \todo{which motivations} for optimizing a model directly on a metric at training time. A general framework for AUC, AUROC and F1 is presented in \cite{optimizableLosses}, but the proposed F1 surrogate remains short of being explicitly derived for stochastic gradient descent. \todo{check again with the authors if I can't get inspired from their work}. Recently, a similar work has been proposed to train a Convolutional Neural Network (CNN) from scratch with a few millions of images and hundreds of labels specifically for multilabel tasks \cite{tencent}. This task is loosely related to object detection, similarly to \cite{multitaskLabelImages} mentioned in the previous paragraph.

\subsection{dynamic thresholding}
\label{sec:org543a744}

\emph{dynamic thresholding} accross classes or examples is an issue as soon as the number of labels to predict is unknown. Certain variants of cross-entropy loss accomodate imbalanced label data  \cite{focalLoss}, but remain agnostic towards the number of labels to predict. Solutions have been tailored to that end, starting with determining an ideal global \emph{threshold} depending on use-cases \cite{threshForF1}, or per-class-thresholding after training \cite{moviePosters} and eventually abstracting the threshold away via a \emph{soft-F1} measure \cite{softF1} \todo{say more about this method}. In the latter two cases, the task is to predict genre from movie posters.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./images/knee.png}
\caption{\label{fig:knee}
ordered per-label cross-entropy predictions for each example (each grey line) with the median (orange) and IQR (green \& blue) over all examples. Determining a global threshold can be related to visually finding the "knee" in that median curve (dotted line)}
\end{figure}

\todo{nicer plot on the right dataset}

The proposed method is positioned in the lineage of \emph{algorithm adaptation}, using \emph{metric as losses} and allowing for \emph{dynamic thresholdig}. 

\section{Sigmoid F1 loss}
\label{sec:org1d3ce39}

For a class of multilayer perceptron \(\mathcal{F}(\cdot ; \Theta): \mathcal{X} \rightarrow \mathcal{Y}\), we consider a special case, where \(\mathbf{x} = \{x_1, ..., x_n\}\). Each observation is attributed one or more classes out of a label set \(\mathbf{l} = \{l_1, ..., l_c\}\). Labels \(y_{i}^{j}\) are available for each observation \(i\) and class \(j\). 

For each observation \(i\), label class probabilities can be defined based on predictions as

\todo{check this formula}

\begin{equation}
\mathbf{p}_{i}=\left\{\begin{array}{ll}\hat{\mathbf{y}} & \text { if } y=1 \\ 1-\hat{\mathbf{y}} & \text { otherwise }\end{array}\right.
\end{equation}

Let \(tp\) and \(fp\) be number of true and false positives respectively. It is necessary to define a bound \(b\), at which a prediction is dichotomized:

$$
 t p=\sum_{i \in Y^{+}} \mathds{1}_{\mathbf{p_i} \geq b} \quad f p=\sum_{i \in Y^{-}} \mathds{1}_{\mathbf{p_i} \geq b} \quad fn = \sum_{i \in Y^{+}} \mathds{1}_{\mathbf{p_i} < b}
 $$

\(\mathds{1}_{\mathbf{p_i} \geq b}\), \(\mathds{1}_{\mathbf{p_i} < b}\) are thus the count of positive and negative predictions at threshold \(b\), 

We also define precision and recall

\begin{equation}
\begin{aligned} P &=\frac{t p}{t p+f p} \\ R &=\frac{t p}{t p+f n}=\frac{t p}{\left|Y^{+}\right|} \end{aligned}
\end{equation}

We can then define \(F_\beta\), which can be expressed as the effectiveness of retrieval with respect to a user who attaches \(\beta\) times as much importance to recall than precision \cite{informationRetrieval}.

\doubt{maybe ignore $F_\beta$ and only mention $F_1$}

\begin{equation}
F_{\beta}=\left(1+\beta^{2}\right) \frac{P \cdot R}{\beta^{2} P+R}
\end{equation}

Or equivalently:

\begin{equation}
\begin{aligned} F_{\beta} &=\left(1+\beta^{2}\right) \frac{t p}{\left(1+\beta^{2}\right) t p+\beta^{2} f n+f p} \\ &=\left(1+\beta^{2}\right) \frac{t p}{\beta^{2}|Y+|+t p+f p} \end{aligned}
\end{equation}

Given the presence of the step indicator function \(\sum \mathds{1}_{\mathbf{p_i} \geq b}\), \(F_\beta\) is not differentiable for gradient based methods. One way of surpassing that problem is to use a smooth surrogate.

\subsection{soft F1 score}
\label{sec:orgf5227ab}

It is possible define a \emph{soft F1} score \cite{softF1} \doubt{can we cite a Medium post?} with smooth confusion matrix entries (i.e. \(tp\), \(fp\) and \(fn\) are not natural numbers anymore):

$$
\tilde{tp}=\sum \hat{\mathbf{y}} \odot \mathbf{y} \quad \tilde{fp} = \sum \hat{\mathbf{y}} \odot (\mathbf{1}- \mathbf{y}) \quad \tilde{fn} = \sum (\mathbf{1} - \hat{\mathbf{y}}) \odot \mathbf{y}
$$

\begin{equation}
\mathcal{L}_{\text {softF1}}= \frac{\tilde{tp}}{2 \tilde{tp}+ \tilde{fn}+ \tilde{fp}}
\end{equation}

\emph{softF1} is 

\subsection{sigmoidF1 score}
\label{sec:orgfed6431}

We define \emph{sigmoidF1}, inspired by the \emph{Maximum F1-score criterion} for automatic mispronounciation detection \cite{sigmoid}. Whereas 
A sigmoid function \(S(u)\)

\begin{equation}
S(u)=\frac{1}{1+\exp (-\beta u)}
\end{equation}

Confusion matrix entries then become

$$
\tilde{tp}=\sum S(\hat{\mathbf{y}}) \odot \mathbf{y} \quad \tilde{fp} = \sum \hat{\mathbf{y}} \odot (\mathbf{1} - \mathbf{y}) \quad \tilde{fn} = \sum (\mathbf{1} - \hat{\mathbf{y}}) \odot \mathbf{y}
$$

\doubt{mention smooth hinge loss} \cite{smoothHinge}


\subsection{Robustness}
\label{sec:org1f81b76}


Similarily to the focal loss, sigmoidF1 loss deals with class imbalance (see \cite{focalLoss}), robustness to outliers \cite{focalLoss}.

\todo{statistical robustness assessment}



\subsection{Evaluation Metrics}
\label{sec:orgd72cc2d}

The metrics described below are a result of a survey of different common practices for measuring accuracy of multilabel prediction. When true positives and false positives are used, recall that \(t p=\sum_{i \in Y^{+}} \mathds{1}_{\mathbf{p_i} \geq b}\) and \(f p=\sum_{i \in Y^{-}} \mathds{1}_{\mathbf{p_i} \geq b}\), and thus a threshold \(b\) must be set. When \(b = 0.5\), as is commonly done [SOURCE HERE], a risk remains that a lot of examples remain without predictions.

Extending \(F_1\) to multi-class binary classification amounts to deciding wether to un/pool classes.
In a first pooled iteration, micro \(F_1\) [SOURCE HERE] equates to creating a single 2x2 confusion matrix for all classes:
$$F_1^{micro} = \frac{\sum tp_c}{2 \sum tp_c + \sum fn_c + \sum fp_c} \quad for \quad c \in C$$

Macro \(F_1\) \cite{threshForF1} amounts to creating one confusion matrix per class:

$$F_1^{macro} = \frac{1}{c} \sum_{j=1}^c F_1$$

\doubt{Do we need to justify optimizing for an F1 surrogate at training time and to then use F1 itself as a metric?}

Weighted macro \(F_1\) \todo{find source} is similar but includes weighing to account for class imbalance, i.e. weighing each class by the number of groundtruth positives.

$$F_1^{weighted} = \frac{1}{c} \sum_{j=1}^c n_j F_1 \quad where \quad n_j = \sum_i \mathds{1}_{\mathbf{y_i^j} = 1}$$

Accuracy is the overall fraction of correctly predicted labels \cite{threshForF1}:

$$
A c c=\frac{t p+t n}{t p+t n+f p+f n}
$$


\subsection*{{\color{red}\bfseries\sffamily TODO} compare to  \cite{lossComp}}
\label{sec:org0c5f594}
\section{datasets}
\label{sec:org7feb22c}

sigmoidF1 is tested across different modalities, namely image, video, sound and text, with a focus on text: the most comparable research was on text data.


Among the three datasets used for benchmarking ML-NET \cite{multitaskLabel}, a cancer hallmark dataset is of sub-entity unilabel nature \cite{cancerHallmarks} (available at \url{https://www.cl.cam.ac.uk/\&sim;sb895/HoC.html}): the research clearly describe a process of annotating several expressions within paper abstracts. The remaining two for chemical exposure \cite{chemExposure} (available at \url{https://figshare.com/articles/Corpus\_and\_Software/4668229}) and diagnosis codes assigment \cite{diagnosisCode} (available at \url{https://physionet.org/works/ICD9CodingofDischargeSummaries}), seem to fit to the entity wide multilabel definition but have a strong hierarchical nature. 

Since ML-Net can be considered as the state-of-the-art in the closest task in text, the three datasets above will be used. The 

Cacncer can be described according to its complexity with different principles, named hallmarks \cite{cancerHallmarks}. A corpus of 1580 PubMed abstracts are manually annotated for 10 hallmarks. This is a sub-entity labelling task and will therefore not be used here.

\begin{center}
\includegraphics[width=.9\linewidth]{./images/cancerHallmarksAnnotation.jpg}
\end{center}


video dataset: \cite{holisticVideoData}

, a dataset for movie posters. Music genre 

music genre, Arxiv publications, medical publications.

In order to test sigmoidF1 on different settings, image, sound and text 


The datasets are namely a movie poster dataset, a toxic comments dataset and a medical publications dataset.


\begin{itemize}
\item Multilabel classification for text \cite{toxicComments}

\item Scenery dataset for images \cite{dataScenery}.

\item movie Posters dataset:
\end{itemize}

\url{https://www.kaggle.com/neha1703/movie-genre-from-its-poster}

pre-scraped: \url{https://www.kaggle.com/neha1703/movie-genre-from-its-poster/discussion/35485} (I removed all jpg's that are empty.)

\subsection*{{\color{red}\bfseries\sffamily TODO} download posters myself, to see if I get more  (see utils in \href{https://github.com/ashrefm/multi-label-soft-f1.git}{here})}
\label{sec:org61b8dcf}
\begin{itemize}
\item \url{https://www.kaggle.com/c/imaterialist-challenge-fashion-2018/data}

\item \url{https://archive.ics.uci.edu/ml/datasets/DeliciousMIL\\\%3A+A+Data+Set+for+Multi-Label+Multi-Instance+Learning+with+Instance+Labels}\#
\end{itemize}


Not what we are looking for:

some datasets have spacially differing labels such as \href{https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/}{Amazon rainforest}.

\emph{citing Kaggle datasets}
\url{https://www.kaggle.com/data/46091}


\section{Experimental Results}
\label{sec:orgedae6c8}


\subsection{implementation}
\label{sec:org85e54b3}

varying b in the sigmoid function as if it is an adaptive learning rate.

one b per class

if we consider \(b\) and \(c\) to be probabilistic, we can then use tensorflow probability to assess their distribution

the batch size has to be relatively large (i.c. 256), in order for meaningful F1 surrogates to be calculated.



\textbf{VanillaResnet}

\begin{array}{cccccc}\hline Loss  & \rotatebox[origin=c]{270}{macroF @ 0.5} & \rotatebox[origin=c]{270}{microF1 @ 0.5} & \rotatebox[origin=c]{270}{weightedF1 @ 0.5} & \rotatebox[origin=c]{270}{Precision @ 0.5} & \rotatebox[origin=c]{270}{Recall @ 0.5}\\ 
\hline \mathcal{L}_{\text {CE}} & 0.057 & 0.200 & 0.159 & 0.106 & 0.106 \\ 
\mathcal{L}_{\text {FL}} & 0.055 & 0.192 & 0.154 & 0.115 & 0.115 \\
\mathcal{L}_{\text {CE+N}} & 0 & 0 & 0 & 0 & 0 \\
\mathcal{L}_{\text {CE+T}} & 0 & 0 & 0 & 0 & 0 \\
\mathcal{L}_{\text {macroSoftF1}} & 0.132 & 0.323 & 0.280 & 0.105 & 0.105 \\
\mathcal{L}_{\text {sigmoidF1}} & \mathbf{0.117} & \mathbf{0.240} & \mathbf{0.263} & \mathbf{0.103} & \mathbf{0.103} \\
\hline\end{array}

\textbf{TencentResnet}

\begin{array}{ccccc}\hline \text { Metric } & \mathcal{L}_{\text {CE}} & \mathcal{L}_{\text {FL}} & \mathcal{L}_{\text {CE+N}} & \mathcal{L}_{\text {CE+T}} \\ 
\hline P(\%) & 0 & 0 & 0 & 0 \\ 
R(\%) & 0 & 0 & 0 & 0 \\
F_{1}(\%) & 0 & 0 & 0 & \mathbf{0} \\
\hline\end{array}


\textbf{DenseNet}

\begin{array}{ccccc}\hline \text { Metric } & \mathcal{L}_{\text {CE}} & \mathcal{L}_{\text {FL}} & \mathcal{L}_{\text {CE+N}} & \mathcal{L}_{\text {CE+T}} \\ 
\hline P(\%) & 0 & 0 & 0 & 0 \\ 
R(\%) & 0 & 0 & 0 & 0 \\
F_{1}(\%) & 0 & 0 & 0 & \mathbf{0} \\
\hline\end{array}

\textbf{some TextNet}

\begin{array}{ccccc}\hline \text { Metric } & \mathcal{L}_{\text {CE}} & \mathcal{L}_{\text {FL}} & \mathcal{L}_{\text {CE+N}} & \mathcal{L}_{\text {CE+T}} \\ 
\hline P(\%) & 0 & 0 & 0 & 0 \\ 
R(\%) & 0 & 0 & 0 & 0 \\
F_{1}(\%) & 0 & 0 & 0 & \mathbf{0} \\
\hline\end{array}

\textbf{simulated data}

\begin{array}{ccccc}\hline \text { Metric } & \mathcal{L}_{\text {CE}} & \mathcal{L}_{\text {FL}} & \mathcal{L}_{\text {CE+N}} & \mathcal{L}_{\text {CE+T}} \\ 
\hline P(\%) & 0 & 0 & 0 & 0 \\ 
R(\%) & 0 & 0 & 0 & 0 \\
F_{1}(\%) & 0 & 0 & 0 & \mathbf{0} \\
\hline\end{array}

\section{future work}
\label{sec:org3b55180}

Apply the loss function to more sophisticated neural network architectures that use F1 score as an evaluation metric such as AC-SUM-GAN \cite{AC-SUM-GAN}.

This model can be adapted for hiarchical multilabel classification or active learning (for both see \cite{activeLearningMultiLabel}).

Combine the proposed loss functions with representation learning \cite{unsupervisedImage,highResRepresentation} or self-supervised learning, in order to model abstract relationships between the labels.

adapt to \emph{extreme} multilabel prediction \cite{extremeMultilabelText}

\section{drawbacks}
\label{sec:orge46cc4e}

it is debatable wether any task is intrinsincly multilabel and wether the image / text cannot be decomposed in parts that are single labelled.

not long training and small models, but aibility to demonstrate the statement anyways.

\begin{acks}}
This work was supported by many people.
All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
\end{acks}

\bibliographystyle{ACM-Reference-Format}


\bibliography{multilabel}
\end{document}
