% Created 2020-12-26 Sat 11:46
% Intended LaTeX compiler: pdflatex
\documentclass[sigconf,natbib,screen=true,review=true,anonymous]{acmart}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
% We'll get the submission number fro the submission system
\acmSubmissionID{xx}
\input{packages}
\input{definitions}
\input{authors}
\input{meta}
\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{TODO : #1}}
\newcommand\doubt[1]{\textcolor{orange}{DOUBT : #1}}
% \newcommand\todo[1]{} % uncomment to hide comments
% \newcommand\doubt[1]{} % uncomment to hide comments
\usepackage{dsfont}
\usepackage{color}
\author{Gabriel Bénédict}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={Gabriel Bénédict},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.1 (Org mode 9.1.14)}, 
 pdflang={English}}
\begin{document}

\title{Loss framework for entity-wide multiclass multilabel prediction with varying number of labels}


\begin{abstract}
Multilabel classification is a common task in text, image or video (scene) prediction.
\end{abstract}


\keywords{Keyword; Keyword; Keyword}

\maketitle

\acresetall

\section{introduction}
\label{sec:org26fb9e7}

As neural network models are able to learn more and more abstract representations via deeper networks, representation learning and self-supervision, it might be reasonable to expect that, thanks to their conferred broader understanding of the world, they get better at predicting more abstract labels. Beyond objects types, face recognition, expressions, neural networks might be able predict genres/categories \todo{other things as well?} of text, image and sound. While researchers are working hard at building neural networks with very high level understandings, there seems to be few research on developing loss functions that are adapted for these higher level concepts in the output space.

Although multilabel binary prediction (commonly referring to mutually inclusive labels) is a task thoroughly covered in existing litterature, there does not seem to exist a framework that deals with different amounts of positive labels in the groundtruth. For example, a scientific journal can be tagged as \emph{machine learning} and \emph{economics}, or a movie can be tagged as \emph{romance} and \emph{comedy}. These instances might as well be assigned only one tag in the groundtruth, or many more within the possible tags (classes).

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./tree/Tree.pdf}
\caption{\label{fig:tree}
"multiclass" nomenclature}
\end{figure}

Before, exploring the subject further, we will use Figure \ref{fig:tree} to disambiguate the terminology used in this research. There seems to exist a concensus over the terms multiclass and multilabel, meaning respectively mutually exclusive and mutually inclusive labels. Multilabel, can therefore be seen as a subdomain of multiclass learning, where more than one class can be true for the same example. Within multilabel training, we introduce the distinction between sub-entity unilabel and entity-wide multilabel. The former refers to tasks where elements within each example can be singled-out (objects in an image or expressions in a text) and assigned a single label. In the countrary, this paper focusses on entity-wide multilabel training with varying label counts. These disctinctions will prove useful troughout the text.

The particularity of tasks like scientific paper tagging or movie genre classification is that it remains unclear what elements in an image/video or text can be singled out as predictive of a particular tag/genre. Rather, a complex interaction between these elements in the feature space steer the predictions. For example, the sole mention of the term "machine learning" in a paper should not be a sufficient condition to tag it as such. Instead, one could expect from the publisher to get acquainted with the paper enough to determine wether the research is a worthwhile contribution or application of \emph{machine learning} to deserve the tag. This involves thorough understanding of the proposed method and background knowledge on state-of-the-art methods. An analogous argument can be made for movie genre classification for movie posters.

However, if elements in an image/text can be singled out as predictive of a single tag, the problem reverts back to predicting with the a priori knowledge of the existence of only one true label. The singled-out elements can be subsets of the original feature space (typically in object detection like with the COCO dataset  \cite{COCO} and \todo{others}). Similarly, it is possible that abstract representations of the feature set would allow for singling out of tasks that predict a single true label in the future  (like GPT-3 \todo{source}) \todo{more examples}. This might also carry prospects of generalizability of the model \cite{generalization}. 

But for now, in certain retrieval tasks such as scientific journal tagging, the predictive power of each feature for each single label remains opaque. In that context, the problem is conveniently framed as multilabel binary prediction with varying amount of positive labels in the groundtruth. The reason for distanciating singling-out from entity-wide labels, is that it has been shown that as soon as singling-out is possible, models that work on instances are more accurate \todo{sources}.

To allow the use of existing diffentiable loss fonctions, previous research papers tend to reframe the problem into either (I) a uni-label top-1 for each feature (as described above, with the COCO dataset as an example of isolation of features \cite{COCO}), (II) a top-1 prediction (III) a top-k prediction with fixed k (IV) a top-k prediction with varying k after training (V) redefine backpropagation for multilabel prediction \cite{multilabelBackprop} (VI) multitask learning \cite{multitaskLabel}. This order reflects how close modelling is to the groundtruth, which remains multilabel with varying amounts of labels. \todo{group them}


\todo{delta with hierarchical labels}

For the problem definitions (III) and (IV)


In a number of retrieval tasks, a model's out of sample accuracy is measured on metrics such as AUROC, F1 score, etc. These reflect an objective catered towards evaluating the model over an entire ranking. Due to to lack of differentiability, these metrics cannot be directly used as loss functions at training time (in-sample). A seminal study \cite{optimizableLosses} derived a general framework for doing so. 

A dataset of multilabel images for visual representation learning was recently released along with its pretrained model. In contrast to most modles trained on image-net \todo{source}, the implementation is trained on a multi-label prediction task from scratch \cite{tencent}.

immediately point out the delta

first identifying elements is the solution to the problem.

Sometimes it seems useful to optimize a neural network directly on the evaluation metric \cite{optimizableLosses}. In the case of multilabel classification with varying amount of labels, common loss functions such as cross-entropy loss or multinomial logit loss deliver predictions on the unit interval. In the case of mutually exclusive labels, this is a viable solution. Sometimes the groundtruth consists of a collection positive and negative valued classes, in varying amounts across observations. In other terms, one is  looking for a top-k prediction with varying \(k\) accross observations.

what is multilabel

RQ: does a contrastive method to deal with varying amount of labels help with predictions (measured by multilabel F1)

contrastive

what is the problem if we don't take into account the number of wrongs and rights.
Find who suggested that this should be done.

top Kappa is more realistic in cases where ranking differs.

Similarily to the focal loss, sigmoidF1 loss deals with class imbalance (see \cite{focalLoss}), robustness to outliers \cite{focalLoss}.

While these recent studies are focussing on top-\(k\) prediction with fixed \(k\), or often top 1. Among different kinds of animals, identify the specie of this animal. Note that there can be several animals (or more generally, objects) on the picture, but the task is still top-1 prediction, couples with an object detection task. i.e. contrasting it from object detection (typically COCO dataset  or text entity recognition (typically this dataset ).

We propose a framework that explicitly deals with varying numbers of positive groundtruth labels per example.


it is not a tag problem, because it is user generated and limitless number of classes


\subsection{our contribution}
\label{sec:orgcd59138}

We propose a general mathematical formulation of multilabel learning for varying amount of groundtruth labels. The generalization encompasses different levels of complexity, from the classical cross-entropy loss up to the proposed loss function. \emph{sigmoidF1} is a F1 score surrogate which allows to optimize for label prediction and count simultanuously in a single task and is robust to outliers. It delivers more precise predictions than the current state-of-the-art on several different metrics, accross text and image related tasks.


\section{Building up on losses}
\label{sec:org3178191}

Multi-label learning can be divided into two major fields: \emph{problem transformation} and \emph{algorithm adaptation} \cite{multilabelReview}. In the former case, multilabel classification is reframed as a binary, multiclass classification or label ranking problem. In the latter, one tries to adapt multiclass algorithms to the problem. The current endeavour focusses on \emph{algorithm adaptation}.


For the purpose of \emph{problem transformation}, we define \(\mathcal{L}_{\text {multiclass}}\), a class of loss functions that minimize predictions in relative terms. Binary cross-entropy, logit and their variants such as focal loss or hinge loss (deemed unstable \cite{focalLoss}) are common choices when it comes to multiclass prediction. Cross-entropy loss can be formulated as \(\mathcal{L}_{\text {CE}}=-\sum \log \left(p_{i}\right)\) . Note that minimizing binary cross-entropy is equivalent to maximizing for log-likelihood \cite[Section 4.3.4]{Bishop}. More generally, the \emph{problem transformation} formulation amounts to minimizing the loss on a class of neural networks, such that

\begin{equation}
\underset{\mathcal{L}_{\text {multiclass}}} {\min} \mathcal{F}\left(\cdot ; \Theta; \mathcal{L}_{\text {multiclass}} (\mathbf{y}, \hat{\mathbf{y}}) \right),
\end{equation}

In the context of \emph{algorithm adaptation}, where the number of positive labels in the groundtruth is unknown a priori, we aim to both obtain a propensity of each label being true and a prediction of the number of true labels: 

\begin{equation}
\underset{\mathcal{L}_{\text {multiclass}}, \mathcal{L}_{\text {count}}} {\min} \mathcal{F}\left(\cdot ; \Theta; \mathcal{L}_{\text {multiclass}} (\mathbf{y}, \hat{\mathbf{y}}) + \lambda \mathcal{L}_{\text {count}} (\mathbf{n}, \hat{\mathbf{n}})\right),
\end{equation}

where \(n_i = \sum_j \mathds{1}_{\mathbf{y_i^j} = 1}\) is the count of positive labels per example. We thus impose a constraint for the retrieval of label counts. For example, a cross-entropy loss surrogate would penalize for the number of wrongly predicted labels \(\mathcal{L}_{\text {CE+N}}= \mathcal{L}_{\text {CE}} + \lambda (\sum tp / \sum p)\), with \(t p=\sum_{i \in Y^{+}} \mathds{1}_{\mathbf{p_i} \geq b}\) and \(b\) a threshold to be defined. \todo{tencent loss}.

This formulation is most straightfoward but suffers from higher parametrization and the lack of modelling of the interactions between label counts and label prediction. To mitigate these issues, we propose a unified loss formulation, namely

\begin{equation}
\underset{\mathcal{L}_{\text {multitag}}} {\min} \mathcal{F}\left(\cdot ; \Theta; \mathcal{L}_{\text {multitag}} (\mathbf{y}, \hat{\mathbf{y}}, \mathbf{n}, \hat{\mathbf{n}}) \right),
\end{equation}

Although predictions and counts explicitely appear in that formulation, \(\mathcal{L}_{\text {multitag}}\) can optimize for both metrics implicitely (see proposed \emph{sigmoidF1} below).


\todo{look at YOU ONLY TRAIN ONCE: LOSS-CONDITIONAL TRAINING OF DEEP NETWORKS}

\todo{cite stat learning}   \cite[p. 308-310]{statLearning}

\section{related work}
\label{sec:orgba5dda7}

\todo{look at [[https://www.sciencedirect.com/topics/computer-science/extractive-summarization][extractive summarization]]}

This section will be guided by the previous section's formulation of the multitags problem, we will therefore focus on \emph{algorithm adaptation}, \emph{metrics as losses} and \emph{dynamic thresholding}.

\subsection{algorithm adaptation}
\label{sec:org94e1c92}

Early representatives of \emph{algorithm adaptation} stem from heterogenous domains of machine learning. Multi-Label k-Nearest Neighbors \cite{ML-KNN}, Multi-Label Decision Tree \cite{ML-DT}, Ranking Support Vector Machine \cite{multilabelSVM} and Backpropagation for Multi-Label Learning \cite{multilabelBackprop}. More recently, two papers introduced the idea of multitask learning for \emph{label prediction} and \emph{label count prediction} for text (ML\(_{\text{NET}}\)) \cite{multitaskLabel} and image \cite{multitaskLabelImages} data. The latter research is loosely catered towards object detection (although not formally presented as such) and is thus out-of-scope: elements in a picture are predicted that tend to be unilabel as defined by the groundtruth (e.g. cat, flower, vase, person, bottle etc.).

\subsection{metrics as losses}
\label{sec:orga1c90b6}

Often, machine learning post-training evaluation metrics (e.g. AUROC, F1) are not differentiable. There are motivations \todo{which motivations} for optimizing a model directly on a metric at training time. A general framework for AUC, AUROC and F1 is presented in \cite{optimizableLosses}, but the proposed F1 surrogate remains short of being explicitly derived for stochastic gradient descent. \todo{check again with the authors if I can't get inspired from their work}. Recently, a similar work has been proposed to train a CNN from scratch with millions of images specifically for multilabel tasks \cite{tencent}. Similarly to \cite{multitaskLabelImages} mentioned in the previous paragraph, this task is loosely related to object detection.

\subsection{dynamic thresholding}
\label{sec:org4d5565b}

\emph{dynamic thresholding} accross classes or examples is an issue as soon as the number of labels to predict is unknown. Certain variants of cross-entropy loss accomodate imbalanced label data  \cite{focalLoss}, but remain agnostic towards the number of labels to predict. Solutions have been tailored to that end, starting with determining an ideal global \emph{threshold} depending on use-cases \cite{threshForF1}, or per-class-thresholding after training \cite{moviePosters} and eventually abstracting the threshold away via a \emph{soft-F1} measure \cite{softF1} \todo{say more about this method}. In the latter two cases, the task is to predict genre from movie posters.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./images/knee.png}
\caption{\label{fig:knee}
ordered per-label cross-entropy predictions for each example (each grey line) with the median (orange) and IQR (green \& blue) over all examples. Determining a global threshold can be related to visually finding the "knee" in that median curve (dotted line)}
\end{figure}

\todo{nicer plot on the right dataset}

The proposed method is positioned in the lineage of \emph{algorithm adaptation}, using \emph{metric as losses} and allowing for \emph{dynamic thresholdig}. 

\section{Sigmoid F1 loss}
\label{sec:orgcf3a92b}

For a class of multilayer perceptron \(\mathcal{F}(\cdot ; \Theta): \mathcal{X} \rightarrow \mathcal{Y}\), we consider a special case, where \(\mathbf{x} = \{x_1, ..., x_n\}\). Each observation is attributed one or more classes out of a label set \(\mathbf{l} = \{l_1, ..., l_c\}\). Labels \(y_{i}^{j}\) are available for each observation \(i\) and class \(j\). 

For each observation \(i\), label class probabilities can be defined based on predictions as

\todo{check this formula}

\begin{equation}
\mathbf{p}_{i}=\left\{\begin{array}{ll}\hat{\mathbf{y}} & \text { if } y=1 \\ 1-\hat{\mathbf{y}} & \text { otherwise }\end{array}\right.
\end{equation}

Let \(tp\) and \(fp\) be number of true and false positives respectively. It is necessary to define a bound \(b\), at which a prediction is dichotomized:

$$
 t p=\sum_{i \in Y^{+}} \mathds{1}_{\mathbf{p_i} \geq b} \quad f p=\sum_{i \in Y^{-}} \mathds{1}_{\mathbf{p_i} \geq b} \quad fn = \sum_{i \in Y^{+}} \mathds{1}_{\mathbf{p_i} < b}
 $$

\(\mathds{1}_{\mathbf{p_i} \geq b}\), \(\mathds{1}_{\mathbf{p_i} < b}\) are thus the count of positive and negative predictions at threshold \(b\), 

We also define precision and recall

\begin{equation}
\begin{aligned} P &=\frac{t p}{t p+f p} \\ R &=\frac{t p}{t p+f n}=\frac{t p}{\left|Y^{+}\right|} \end{aligned}
\end{equation}

We can then define \(F_\beta\), which can be expressed as the effectiveness of retrieval with respect to a user who attaches \(\beta\) times as much importance to recall than precision \cite{informationRetrieval}.

\doubt{maybe ignore $F_\beta$ and only mention $F_1$}

\begin{equation}
F_{\beta}=\left(1+\beta^{2}\right) \frac{P \cdot R}{\beta^{2} P+R}
\end{equation}

Or equivalently:

\begin{equation}
\begin{aligned} F_{\beta} &=\left(1+\beta^{2}\right) \frac{t p}{\left(1+\beta^{2}\right) t p+\beta^{2} f n+f p} \\ &=\left(1+\beta^{2}\right) \frac{t p}{\beta^{2}|Y+|+t p+f p} \end{aligned}
\end{equation}

Given the presence of the step indicator function \(\sum \mathds{1}_{\mathbf{p_i} \geq b}\), \(F_\beta\) is not differentiable for gradient based methods. One way of surpassing that problem is to use a smooth surrogate.

\subsection{soft F1 score}
\label{sec:org0604bae}

It is possible define a \emph{soft F1} score \cite{softF1} \doubt{can we cite a Medium post?} with smooth confusion matrix entries (i.e. \(tp\), \(fp\) and \(fn\) are not natural numbers anymore):

$$
\tilde{tp}=\sum \hat{\mathbf{y}} \odot \mathbf{y} \quad \tilde{fp} = \sum \hat{\mathbf{y}} \odot (\mathbf{1}- \mathbf{y}) \quad \tilde{fn} = \sum (\mathbf{1} - \hat{\mathbf{y}}) \odot \mathbf{y}
$$

\begin{equation}
\mathcal{L}_{\text {softF1}}= \frac{\tilde{tp}}{2 \tilde{tp}+ \tilde{fn}+ \tilde{fp}}
\end{equation}

\emph{softF1} is 

\subsection{sigmoidF1 score}
\label{sec:org83b3ba5}

We define \emph{sigmoidF1}, inspired by the \emph{Maximum F1-score criterion} for automatic mispronounciation detection \cite{sigmoid}. Whereas 
A sigmoid function \(S(u)\)

\begin{equation}
S(u)=\frac{1}{1+\exp (-\beta u)}
\end{equation}

Confusion matrix entries then become

$$
\tilde{tp}=\sum S(\hat{\mathbf{y}}) \odot \mathbf{y} \quad \tilde{fp} = \sum \hat{\mathbf{y}} \odot (\mathbf{1} - \mathbf{y}) \quad \tilde{fn} = \sum (\mathbf{1} - \hat{\mathbf{y}}) \odot \mathbf{y}
$$


\doubt{mention smooth hinge loss} \cite{smoothHinge}

\subsection{Evaluation Metrics}
\label{sec:orgf52b7d6}

The metrics described below are a result of a survey of different common practices for measuring accuracy of multilabel prediction. When true positives and false positives are used, recall that \(t p=\sum_{i \in Y^{+}} \mathds{1}_{\mathbf{p_i} \geq b}\) and \(f p=\sum_{i \in Y^{-}} \mathds{1}_{\mathbf{p_i} \geq b}\), and thus a threshold \(b\) must be set. When \(b = 0.5\), as is commonly done [SOURCE HERE], a risk remains that a lot of examples remain without predictions.

Extending \(F_1\) to multi-class binary classification amounts to deciding wether to un/pool classes.
In a first pooled iteration, micro \(F_1\) [SOURCE HERE] equates to creating a single 2x2 confusion matrix for all classes:
$$F_1^{micro} = \frac{\sum tp_c}{2 \sum tp_c + \sum fn_c + \sum fp_c} \quad for \quad c \in C$$

Macro \(F_1\) \cite{threshForF1} amounts to creating one confusion matrix per class:

$$F_1^{macro} = \frac{1}{c} \sum_{j=1}^c F_1$$

\doubt{Do we need to justify optimizing for an F1 surrogate at training time and to then use F1 itself as a metric?}

Weighted macro \(F_1\) [SOURCE HERE] is similar but includes weighing to account for class imbalance, i.e. weighing each class by the number of groundtruth positives.

$$F_1^{weighted} = \frac{1}{c} \sum_{j=1}^c n_j F_1 \quad where \quad n_j = \sum_i \mathds{1}_{\mathbf{y_i^j} = 1}$$

Accuracy is the overall fraction of correctly predicted labels \cite{threshForF1}:

$$
A c c=\frac{t p+t n}{t p+t n+f p+f n}
$$


\subsection*{{\color{red}\bfseries\sffamily TODO} compare to  \cite{lossComp}}
\label{sec:org558565e}
\section{datasets}
\label{sec:orga7de626}

sigmoidF1 is tested across different modalities, namely image sound and text, with a focus on text: the most comparable research was on text data.

\doubt{optional paragraph}
In light of the problem definition leading to the sigmoidF1 framework in the introduction and in order to clearly delimit the proposed method, following are a few datasets that are not suitable for the task.


Among the three datacets used for benchmarking ML-NET \cite{multitaskLabel}, a cancer hallmark dataset is of sub-entity unilabel nature \cite{cancerHallmarks}: the research clearly describe a process of annotating several expressions within paper abstracts. The remaining two, seem to fit to the entity wide multilabel definition and have a strong hierarchical nature.


Cacncer can be described according to its complexity with different principles, named hallmarks \cite{cancerHallmarks}. A corpus of 1580 PubMed abstracts are manually annotated for 10 hallmarks. This is a sub-entity labelling task and will therefore not be used here.

\begin{center}
\includegraphics[width=.9\linewidth]{./images/cancerHallmarksAnnotation.jpg}
\end{center}




, a dataset for movie posters. Music genre 

music genre, Arxiv publications, medical publications.

In order to test sigmoidF1 on different settings, image, sound and text 


The datasets are namely a movie poster dataset, a toxic comments dataset and a medical publications dataset.


\begin{itemize}
\item Multilabel classification for text \cite{toxicComments}

\item Scenery dataset for images \cite{dataScenery}.

\item movie Posters dataset:
\end{itemize}

\url{https://www.kaggle.com/neha1703/movie-genre-from-its-poster}

pre-scraped: \url{https://www.kaggle.com/neha1703/movie-genre-from-its-poster/discussion/35485} (I removed all jpg's that are empty.)

\subsection*{{\color{red}\bfseries\sffamily TODO} download posters myself, to see if I get more  (see utils in \href{https://github.com/ashrefm/multi-label-soft-f1.git}{here})}
\label{sec:org57b158a}
\begin{itemize}
\item \url{https://www.kaggle.com/c/imaterialist-challenge-fashion-2018/data}

\item \url{https://archive.ics.uci.edu/ml/datasets/DeliciousMIL\\\%3A+A+Data+Set+for+Multi-Label+Multi-Instance+Learning+with+Instance+Labels}\#
\end{itemize}


Not what we are looking for:

some datasets have spacially differing labels such as \href{https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/}{Amazon rainforest}.

\emph{citing Kaggle datasets}
\url{https://www.kaggle.com/data/46091}

\section{Experimental Results}
\label{sec:org2fcc5d2}


\subsection{implementation}
\label{sec:org567c4e3}

varying b in the sigmoid function as if it is an adaptive learning rate.

one b per class

if we consider \(b\) and \(c\) to be probabilistic, we can then use tensorflow probability to assess their distribution

the batch size has to be relatively large (i.c. 256), in order for meaningful F1 surrogates to be calculated.



\textbf{VanillaResnet}

\begin{array}{cccccc}\hline Loss  & \rotatebox[origin=c]{270}{macroF @ 0.5} & \rotatebox[origin=c]{270}{microF1 @ 0.5} & \rotatebox[origin=c]{270}{weightedF1 @ 0.5} & \rotatebox[origin=c]{270}{Precision @ 0.5} & \rotatebox[origin=c]{270}{Recall @ 0.5}\\ 
\hline \mathcal{L}_{\text {CE}} & 0.057 & 0.200 & 0.159 & 0.106 & 0.106 \\ 
\mathcal{L}_{\text {FL}} & 0.055 & 0.192 & 0.154 & 0.115 & 0.115 \\
\mathcal{L}_{\text {CE+N}} & 0 & 0 & 0 & 0 & 0 \\
\mathcal{L}_{\text {CE+T}} & 0 & 0 & 0 & 0 & 0 \\
\mathcal{L}_{\text {macroSoftF1}} & 0.132 & 0.323 & 0.280 & 0.105 & 0.105 \\
\mathcal{L}_{\text {sigmoidF1}} & \mathbf{0.117} & \mathbf{0.240} & \mathbf{0.263} & \mathbf{0.103} & \mathbf{0.103} \\
\hline\end{array}

\textbf{TencentResnet}

\begin{array}{ccccc}\hline \text { Metric } & \mathcal{L}_{\text {CE}} & \mathcal{L}_{\text {FL}} & \mathcal{L}_{\text {CE+N}} & \mathcal{L}_{\text {CE+T}} \\ 
\hline P(\%) & 0 & 0 & 0 & 0 \\ 
R(\%) & 0 & 0 & 0 & 0 \\
F_{1}(\%) & 0 & 0 & 0 & \mathbf{0} \\
\hline\end{array}


\textbf{DenseNet}

\begin{array}{ccccc}\hline \text { Metric } & \mathcal{L}_{\text {CE}} & \mathcal{L}_{\text {FL}} & \mathcal{L}_{\text {CE+N}} & \mathcal{L}_{\text {CE+T}} \\ 
\hline P(\%) & 0 & 0 & 0 & 0 \\ 
R(\%) & 0 & 0 & 0 & 0 \\
F_{1}(\%) & 0 & 0 & 0 & \mathbf{0} \\
\hline\end{array}

\textbf{some TextNet}

\begin{array}{ccccc}\hline \text { Metric } & \mathcal{L}_{\text {CE}} & \mathcal{L}_{\text {FL}} & \mathcal{L}_{\text {CE+N}} & \mathcal{L}_{\text {CE+T}} \\ 
\hline P(\%) & 0 & 0 & 0 & 0 \\ 
R(\%) & 0 & 0 & 0 & 0 \\
F_{1}(\%) & 0 & 0 & 0 & \mathbf{0} \\
\hline\end{array}

\textbf{simulated data}

\begin{array}{ccccc}\hline \text { Metric } & \mathcal{L}_{\text {CE}} & \mathcal{L}_{\text {FL}} & \mathcal{L}_{\text {CE+N}} & \mathcal{L}_{\text {CE+T}} \\ 
\hline P(\%) & 0 & 0 & 0 & 0 \\ 
R(\%) & 0 & 0 & 0 & 0 \\
F_{1}(\%) & 0 & 0 & 0 & \mathbf{0} \\
\hline\end{array}

\section{future work}
\label{sec:orga9aeb23}

Apply the loss function to more sophisticated neural network architectures that use F1 score as an evaluation metric such as AC-SUM-GAN \cite{AC-SUM-GAN}.

This model can be adapted for hiarchical multilabel classification or active learning (for both see \cite{activeLearningMultiLabel}).

Combine the proposed loss functions with representation learning \cite{unsupervisedImage,highResRepresentation} or self-supervised learning, in order to model abstract relationships between the labels.

adapt to \emph{extreme} multilabel prediction \cite{extremeMultilabelText}

\section{drawbacks}
\label{sec:org4a2a96f}

it is debatable wether any task is intrinsincly multilabel and wether the image / text cannot be decomposed in parts that are single labelled.

not long training and small models, but aibility to demonstrate the statement anyways.

\begin{acks}}
This work was supported by many people.
All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
\end{acks}

\bibliographystyle{ACM-Reference-Format}


\bibliography{multilabel}
\end{document}
