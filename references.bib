%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Maarten de Rijke at 2020-12-31 11:01:27 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{lossComp,
	author = {Hichame Yessou AND Gencer Sumbul AND Beg{\"u}m Demir},
	date-modified = {2020-12-31 10:56:22 +0100},
	journal = {arXiv preprint arXiv:2009.13935v1},
	title = {{A Comparative Study of Deep Learning Loss Functions for Multi-Label Remote Sensing Image Classification}},
	year = 2020}

@misc{toxicComments,
	abstractnote = {This data set includes over 100k labeled discussion comments from English Wikipedia. Each comment was labeled by multiple annotators via Crowdflower on whether it is a toxic or healthy contribution. We also include some demographic data for each crowd-worker. See our wiki for documentation of the schema of each file and our research paper for documentation on the data collection and modeling methodology. For a quick demo of how to use the data for model building and analysis, check out this ipython notebook.},
	author = {Thain, Nithum and Dixon, Lucas and Wulczyn, Ellery},
	doi = {10.6084/m9.figshare.4563973.v2},
	month = {Feb},
	publisher = {figshare},
	title = {Wikipedia Talk Labels: Toxicity},
	url = {https://figshare.com/articles/dataset/Wikipedia_Talk_Labels_Toxicity/4563973/2},
	year = {2017},
	Bdsk-Url-1 = {https://figshare.com/articles/dataset/Wikipedia_Talk_Labels_Toxicity/4563973/2},
	Bdsk-Url-2 = {https://doi.org/10.6084/m9.figshare.4563973.v2}}

@article{dataScenery,
	doi = {10.7551/mitpress/7503.003.0206},
	isbn = 9780262256919,
	journal = {Advances in Neural Information Processing Systems 19},
	publisher = {The MIT Press},
	url = {http://dx.doi.org/10.7551/mitpress/7503.003.0206},
	year = 2007,
	Bdsk-Url-1 = {http://dx.doi.org/10.7551/mitpress/7503.003.0206}}

@article{statLearning,
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	doi = {10.1007/978-0-387-84858-7},
	isbn = 9780387848587,
	issn = {2197-568X},
	journal = {Springer Series in Statistics},
	publisher = {Springer New York},
	title = {The Elements of Statistical Learning},
	url = {http://dx.doi.org/10.1007/978-0-387-84858-7},
	year = 2009,
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-0-387-84858-7}}

@article{optimizableLosses,
	author = {Elad ET. Eban AND Mariano Schain AND Alan Mackey AND Ariel Gordon AND Rif A. Saurous AND Gal Elidan},
	date-modified = {2020-12-31 10:55:24 +0100},
	journal = {arXiv preprint arXiv:1608.04802v2},
	title = {{Scalable Learning of Non-Decomposable Objectives}},
	year = 2016}

@article{sigmoid,
	author = {Huang, Hao and Xu, Haihua and Wang, Xianhui and Silamu, Wushour},
	doi = {10.1109/taslp.2015.2409733},
	issn = {2329-9304},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	month = {Apr},
	number = 4,
	pages = {787--797},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	title = {Maximum F1-Score Discriminative Training Criterion for Automatic Mispronunciation Detection},
	url = {http://dx.doi.org/10.1109/taslp.2015.2409733},
	volume = 23,
	year = 2015,
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/taslp.2015.2409733}}

@article{smoothHinge,
	author = {Rennie, Jason DM},
	date-modified = {2020-12-31 10:58:50 +0100},
	journal = {Proceeding of Massachusetts Institute of Technology},
	title = {Smooth Hinge Classification},
	year = 2005}

@article{AC-SUM-GAN,
	author = {E. Apostolidis and E. Adamantidou and A. I. Metsai and V. Mezaris and I. Patras},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	title = {AC-SUM-GAN: Connecting Actor-Critic and Generative Adversarial Networks for Unsupervised Video Summarization,},
	year = 2020}

@article{activeLearningMultiLabel,
	abstract = {Due to technological advances, a massive amount of data is produced daily, presenting challenges for application areas where data needs to be labelled by a domain specialist or by expensive procedures, in order to be useful for supervised machine learning purposes. In order to select which data points will provide more information when labelled, one can make use of active learning methods. Active learning (AL) is a subfield of machine learning which addresses methods to build models with fewer, but more representative instances. Even though AL has been vastly studied, it has not been thoroughly investigated in hierarchical multi-label classification, a learning task where multiple class labels can be assigned to an instance and these labels are hierarchically structured. In this work, we provide a public framework containing baseline and state-of-the-art algorithms suitable for this task. Additionally, we also propose a new algorithm, namely Hierarchical Query-By-Committee (H-QBC), which is validated on datasets from different domains. Our results show that H-QBC is capable of providing superior predictive performance results compared to its competitors, while being computationally efficient and parameter free.},
	author = {Nakano, Felipe Kenji and Cerri, Ricardo and Vens, Celine},
	da = {2020/09/01},
	date-added = {2020-12-13 18:23:52 +0000},
	date-modified = {2020-12-31 10:59:13 +0100},
	doi = {10.1007/s10618-020-00704-w},
	id = {Nakano2020},
	isbn = {1573-756X},
	journal = {Data Mining and Knowledge Discovery},
	number = {5},
	pages = {1496--1530},
	title = {Active Learning for Hierarchical Multi-label Classification},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s10618-020-00704-w},
	volume = {34},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10618-020-00704-w},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/s10618-020-00704-w}}

@article{focalLoss,
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
	doi = {10.1109/iccv.2017.324},
	isbn = 9781538610329,
	journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
	month = {Oct},
	publisher = {IEEE},
	title = {Focal Loss for Dense Object Detection},
	url = {http://dx.doi.org/10.1109/iccv.2017.324},
	year = 2017,
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/iccv.2017.324}}

@article{tencent,
	author = {Wu, Baoyuan and Chen, Weidong and Fan, Yanbo and Zhang, Yong and Hou, Jinlong and Liu, Jie and Zhang, Tong},
	doi = {10.1109/access.2019.2956775},
	issn = {2169-3536},
	journal = {IEEE Access},
	pages = {172683--172693},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	title = {Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning},
	url = {http://dx.doi.org/10.1109/access.2019.2956775},
	volume = 7,
	year = 2019,
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/access.2019.2956775}}

@article{COCO,
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
	doi = {10.1007/978-3-319-10602-1_48},
	isbn = 9783319106021,
	issn = {1611-3349},
	journal = {Lecture Notes in Computer Science},
	pages = {740--755},
	publisher = {Springer International Publishing},
	title = {Microsoft COCO: Common Objects in Context},
	url = {http://dx.doi.org/10.1007/978-3-319-10602-1_48},
	year = 2014,
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-319-10602-1_48}}

@article{threshForF1,
	author = {Lipton, Zachary C. and Elkan, Charles and Naryanaswamy, Balakrishnan},
	doi = {10.1007/978-3-662-44851-9_15},
	isbn = 9783662448519,
	issn = {1611-3349},
	journal = {Lecture Notes in Computer Science},
	pages = {225--239},
	publisher = {Springer Berlin Heidelberg},
	title = {Optimal Thresholding of Classifiers to Maximize F1 Measure},
	url = {http://dx.doi.org/10.1007/978-3-662-44851-9_15},
	year = 2014,
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-662-44851-9_15}}

@article{generalization,
	author = {Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
	date-modified = {2020-12-31 10:57:34 +0100},
	journal = {arXiv preprint arXiv:1611.03530},
	title = {Understanding Deep Learning Requires Rethinking Generalization},
	year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1611.03530}}

@article{multilabelBackprop,
	author = {Min-Ling Zhang and Zhi-Hua Zhou},
	doi = {10.1109/tkde.2006.162},
	issn = {1041-4347},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	month = {Oct},
	number = 10,
	pages = {1338--1351},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	title = {Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization},
	url = {http://dx.doi.org/10.1109/tkde.2006.162},
	volume = 18,
	year = 2006,
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/tkde.2006.162}}

@article{multitaskLabel,
	author = {Du, Jingcheng and Chen, Qingyu and Peng, Yifan and Xiang, Yang and Tao, Cui and Lu, Zhiyong},
	date-modified = {2020-12-31 11:00:46 +0100},
	doi = {10.1093/jamia/ocz085},
	issn = {1527-974X},
	journal = {Journal of the American Medical Informatics Association},
	month = {Jun},
	number = 11,
	pages = {1279--1285},
	publisher = {Oxford University Press (OUP)},
	title = {ML-Net: Multi-label Classification of Biomedical Texts with Deep Neural Networks},
	url = {http://dx.doi.org/10.1093/jamia/ocz085},
	volume = 26,
	year = 2019,
	Bdsk-Url-1 = {http://dx.doi.org/10.1093/jamia/ocz085}}

@article{multilabelReview,
	author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
	doi = {10.1109/tkde.2013.39},
	issn = {1041-4347},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	month = {Aug},
	number = 8,
	pages = {1819--1837},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	title = {A Review on Multi-Label Learning Algorithms},
	url = {http://dx.doi.org/10.1109/tkde.2013.39},
	volume = 26,
	year = 2014,
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/tkde.2013.39}}

@book{Bishop,
	author = {Christopher M. Bishop},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/books/lib/Bishop07.bib},
	date-modified = {2020-12-31 11:01:09 +0100},
	edition = {5},
	isbn = 9780387310732,
	publisher = {Springer},
	series = {Information science and statistics},
	timestamp = {Fri, 17 Jul 2020 16:12:42 +0200},
	title = {Pattern Recognition and Machine Learning},
	url = {https://www.worldcat.org/oclc/71008143},
	year = 2007,
	Bdsk-Url-1 = {https://www.worldcat.org/oclc/71008143}}

@article{ML-KNN,
	author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
	date-modified = {2020-12-31 10:56:52 +0100},
	doi = {10.1016/j.patcog.2006.12.019},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	month = {Jul},
	number = 7,
	pages = {2038--2048},
	publisher = {Elsevier BV},
	title = {ML-KNN: A Lazy Learning Approach to Multi-label Learning},
	url = {http://dx.doi.org/10.1016/j.patcog.2006.12.019},
	volume = 40,
	year = 2007,
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.patcog.2006.12.019}}

@article{ML-DT,
	author = {Clare, Amanda and King, Ross D.},
	doi = {10.1007/3-540-44794-6_4},
	isbn = 9783540447948,
	issn = {0302-9743},
	journal = {Lecture Notes in Computer Science},
	pages = {42--53},
	publisher = {Springer Berlin Heidelberg},
	title = {Knowledge Discovery in Multi-label Phenotype Data},
	url = {http://dx.doi.org/10.1007/3-540-44794-6_4},
	year = 2001,
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/3-540-44794-6_4}}

@inproceedings{multilabelSVM,
	abstract = {This article presents a Support Vector Machine (SVM) like learning system to handle multi-label problems. Such problems are usually decomposed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common properties with SVMs. We tested it on a Yeast gene functional classification problem with positive results.},
	address = {Cambridge, MA, USA},
	author = {Elisseeff, Andr\'{e} and Weston, Jason},
	booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
	location = {Vancouver, British Columbia, Canada},
	numpages = {7},
	pages = {681--687},
	publisher = {MIT Press},
	series = {NIPS'01},
	title = {A Kernel Method for Multi-Labelled Classification},
	year = {2001}}

@article{multitaskLabelImages,
	author = {Li, Yuncheng and Song, Yale and Luo, Jiebo},
	doi = {10.1109/cvpr.2017.199},
	isbn = 9781538604571,
	journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {Jul},
	publisher = {IEEE},
	title = {Improving Pairwise Ranking for Multi-label Image Classification},
	url = {http://dx.doi.org/10.1109/cvpr.2017.199},
	year = 2017,
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/cvpr.2017.199}}

@article{unsupervisedImage,
	author = {Milbich, Timo and Ghori, Omair and Diego, Ferran and Ommer, Bj{\"o}rn},
	date-modified = {2020-12-31 10:59:27 +0100},
	doi = {10.1016/j.patcog.2019.107107},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	month = {Jun},
	pages = 107107,
	publisher = {Elsevier BV},
	title = {Unsupervised Representation Learning by Discovering Reliable Image Relations},
	url = {http://dx.doi.org/10.1016/j.patcog.2019.107107},
	volume = 102,
	year = 2020,
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.patcog.2019.107107}}

@article{highResRepresentation,
	author = {Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and et al.},
	doi = {10.1109/tpami.2020.2983686},
	issn = {1939-3539},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {1--1},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	title = {Deep High-Resolution Representation Learning for Visual Recognition},
	url = {http://dx.doi.org/10.1109/tpami.2020.2983686},
	year = 2020,
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/tpami.2020.2983686}}

@article{moviePosters,
	author = {Chu, Wei-Ta and Guo, Hung-Jui},
	doi = {10.1145/3132515.3132516},
	isbn = 9781450355094,
	journal = {Proceedings of the Workshop on Multimodal Understanding of Social, Affective and Subjective Attributes},
	month = {Oct},
	publisher = {ACM},
	title = {Movie Genre Classification based on Poster Images with Deep Neural Networks},
	url = {http://dx.doi.org/10.1145/3132515.3132516},
	year = 2017,
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/3132515.3132516}}

@article{cancerHallmarks,
	author = {Hanahan, Douglas and Weinberg, Robert A.},
	doi = {10.1016/j.cell.2011.02.013},
	issn = {0092-8674},
	journal = {Cell},
	month = {Mar},
	number = 5,
	pages = {646--674},
	publisher = {Elsevier BV},
	title = {Hallmarks of Cancer: The Next Generation},
	url = {http://dx.doi.org/10.1016/j.cell.2011.02.013},
	volume = 144,
	year = 2011,
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.cell.2011.02.013}}

@article{cancerHallmarksAnnotation,
	author = {Baker, Simon and Silins, Ilona and Guo, Yufan and Ali, Imran and H{\"o}gberg, Johan and Stenius, Ulla and Korhonen, Anna},
	date-modified = {2020-12-31 11:01:27 +0100},
	doi = {10.1093/bioinformatics/btv585},
	issn = {1460-2059},
	journal = {Bioinformatics},
	month = {Oct},
	number = 3,
	pages = {432--440},
	publisher = {Oxford University Press (OUP)},
	title = {Automatic Semantic Classification of Scientific Literature According to the Hallmarks of Cancer},
	url = {http://dx.doi.org/10.1093/bioinformatics/btv585},
	volume = 32,
	year = 2015,
	Bdsk-Url-1 = {http://dx.doi.org/10.1093/bioinformatics/btv585}}

@article{extremeMultilabelText,
	author = {Chang, Wei-Cheng and Yu, Hsiang-Fu and Zhong, Kai and Yang, Yiming and Dhillon, Inderjit S.},
	date-modified = {2020-12-28 17:19:12 +0100},
	doi = {10.1145/3394486.3403368},
	isbn = 9781450379984,
	journal = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	month = {Jul},
	publisher = {ACM},
	title = {Taming Pretrained Transformers for Extreme Multi-label Text Classification},
	url = {http://dx.doi.org/10.1145/3394486.3403368},
	year = 2020,
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/3394486.3403368}}

@book{informationRetrieval,
	address = {London},
	author = {Rijsbergen, C.J. van},
	booktitle = {Information retrieval},
	date-modified = {2020-12-31 10:58:40 +0100},
	edition = {2nd [rev.] ed.},
	isbn = {0408709294},
	keywords = {Information storage and retrieval systems},
	language = {eng},
	publisher = {Butterworths},
	title = {Information Retrieval},
	year = {1979}}

@article{softF1,
	author = {Chang, Wei-Cheng and Yu, Hsiang-Fu and Zhong, Kai and Yang, Yiming and Dhillon, Inderjit S.},
	journal = {Towards Data Science},
	month = {Dec},
	publisher = {Medium},
	title = {The Unknown Benefits of using a Soft-F1 Loss in Classification Systems},
	url = {https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d},
	year = 2019,
	Bdsk-Url-1 = {https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d}}

@article{holisticImageDescriptors,
	author = {Jaenal, Alberto and Moreno, Francisco-Angel and Gonzalez-Jimenez, Javier},
	date-modified = {2020-12-31 11:00:00 +0100},
	doi = {10.1145/3309772.3309800},
	isbn = 9781450360852,
	journal = {Proceedings of the 2nd International Conference on Applications of Intelligent Systems - APPIS '19},
	publisher = {ACM Press},
	title = {Experimental Study of the Suitability of CNN-based Holistic Descriptors for Accurate Visual Localization},
	url = {http://dx.doi.org/10.1145/3309772.3309800},
	year = 2019,
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/3309772.3309800}}

@article{holisticLungs,
	author = {Gao, Mingchen and Bagci, Ulas and Lu, Le and Wu, Aaron and Buty, Mario and Shin, Hoo-Chang and Roth, Holger and Papadakis, Georgios Z. and Depeursinge, Adrien and Summers, Ronald M. and et al.},
	date-modified = {2020-12-31 11:00:24 +0100},
	doi = {10.1080/21681163.2015.1124249},
	issn = {2168-1171},
	journal = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging \& Visualization},
	month = {Jun},
	number = 1,
	pages = {1--6},
	publisher = {Informa UK Limited},
	title = {Holistic Classification of CT Attenuation Patterns for Interstitial Lung Diseases via Deep Convolutional Neural Networks},
	url = {http://dx.doi.org/10.1080/21681163.2015.1124249},
	volume = 6,
	year = 2016,
	Bdsk-Url-1 = {http://dx.doi.org/10.1080/21681163.2015.1124249}}

@article{holisticVideoData,
	author = {Ali Diba AND Mohsen Fayyaz AND Vivek Sharma AND Manohar Paluri AND Jurgen Gall AND Rainer Stiefelhagen AND Luc Van Gool},
	date-modified = {2020-12-31 10:56:12 +0100},
	journal = {arXiv preprint arXiv:1904.11451v3},
	title = {{Large Scale Holistic Video Understanding}},
	year = 2019}

@article{chemExposure,
	author = {Larsson, Kristin and Silins, Ilona and Guo, Yufan and Korhonen, Anna and Stenius, Ulla and Berglund, Marika},
	date-modified = {2020-12-31 10:59:43 +0100},
	doi = {10.1016/j.toxlet.2014.06.427},
	issn = {0378-4274},
	journal = {Toxicology Letters},
	month = {Sep},
	pages = {S119},
	publisher = {Elsevier BV},
	title = {Text Mining for Improved Human Exposure Assessment},
	url = {http://dx.doi.org/10.1016/j.toxlet.2014.06.427},
	volume = 229,
	year = 2014,
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.toxlet.2014.06.427}}

@article{diagnosisCode,
	author = {Perotte, Adler and Pivovarov, Rimma and Natarajan, Karthik and Weiskopf, Nicole and Wood, Frank and Elhadad, No{\'e}mie},
	date-modified = {2020-12-31 10:59:02 +0100},
	doi = {10.1136/amiajnl-2013-002159},
	issn = {1527-974X},
	journal = {Journal of the American Medical Informatics Association},
	month = {Mar},
	number = 2,
	pages = {231--237},
	publisher = {Oxford University Press (OUP)},
	title = {Diagnosis Code Assignment: Models and Evaluation Metrics},
	url = {http://dx.doi.org/10.1136/amiajnl-2013-002159},
	volume = 21,
	year = 2014,
	Bdsk-Url-1 = {http://dx.doi.org/10.1136/amiajnl-2013-002159}}

@article{multiInstance,
	author = {H. {Soleimani} and D. J. {Miller}},
	doi = {10.1162/NECO_a_00939},
	journal = {Neural Computation},
	number = {4},
	pages = {1053-1102},
	title = {Semisupervised, Multilabel, Multi-Instance Learning for Structured Data},
	volume = {29},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1162/NECO_a_00939}}

@inproceedings{multiclass,
 author = {Jin, Rong and Ghahramani, Zoubin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Becker and S. Thrun and K. Obermayer},
 pages = {921--928},
 publisher = {MIT Press},
 title = {Learning with Multiple Labels},
 url = {https://proceedings.neurips.cc/paper/2002/file/653ac11ca60b3e021a8c609c7198acfc-Paper.pdf},
 volume = {15},
 year = {2003}
}

@article{multiInstanceMultiLabel,
title = "Multi-instance multi-label learning",
journal = "Artificial Intelligence",
volume = "176",
number = "1",
pages = "2291 - 2320",
year = "2012",
issn = "0004-3702",
doi = "https://doi.org/10.1016/j.artint.2011.10.002",
url = "http://www.sciencedirect.com/science/article/pii/S0004370211001123",
author = "Zhi-Hua Zhou and Min-Ling Zhang and Sheng-Jun Huang and Yu-Feng Li",
keywords = "Machine learning, Multi-instance multi-label learning, MIML, Multi-label learning, Multi-instance learning",
abstract = "In this paper, we propose the MIML (Multi-Instance Multi-Label learning) framework where an example is described by multiple instances and associated with multiple class labels. Compared to traditional learning frameworks, the MIML framework is more convenient and natural for representing complicated objects which have multiple semantic meanings. To learn from MIML examples, we propose the MimlBoost and MimlSvm algorithms based on a simple degeneration strategy, and experiments show that solving problems involving complicated objects with multiple semantic meanings in the MIML framework can lead to good performance. Considering that the degeneration process may lose information, we propose the D-MimlSvm algorithm which tackles MIML problems directly in a regularization framework. Moreover, we show that even when we do not have access to the real objects and thus cannot capture more information from real objects by using the MIML representation, MIML is still useful. We propose the InsDif and SubCod algorithms. InsDif works by transforming single-instances into the MIML representation for learning, while SubCod works by transforming single-label examples into the MIML representation for learning. Experiments show that in some tasks they are able to achieve better performance than learning the single-instances or single-label examples directly."
}

@INPROCEEDINGS{HARAM,
  author={F. {Benites} and E. {Sapozhnikova}},
  booktitle={2015 IEEE International Conference on Data Mining Workshop (ICDMW)}, 
  title={HARAM: A Hierarchical ARAM Neural Network for Large-Scale Text Classification}, 
  year={2015},
  volume={},
  number={},
  pages={847-854},
  doi={10.1109/ICDMW.2015.14}
  }


@Article{MLTSVM,
  author       = {Chen, Wei-Jie and Shao, Yuan-Hai and Li, Chun-Na and
                  Deng, Nai-Yang},
  title        = {MLTSVM: A novel twin support vector machine to
                  multi-label learning},
  year         = 2016,
  volume       = 52,
  month        = {Apr},
  pages        = {61–74},
  issn         = {0031-3203},
  doi          = {10.1016/j.patcog.2015.10.008},
  url          = {http://dx.doi.org/10.1016/j.patcog.2015.10.008},
  journal      = {Pattern Recognition},
  publisher    = {Elsevier BV}
}

@Article{MLTSVMThreeway,
  author       = {Zhang, Yuanjian and Miao, Duoqian and Zhang, Zhifei
                  and Xu, Jianfeng and Luo, Sheng},
  title        = {A three-way selective ensemble model for multi-label
                  classification},
  year         = 2018,
  volume       = 103,
  month        = {Dec},
  pages        = {394–413},
  issn         = {0888-613X},
  doi          = {10.1016/j.ijar.2018.10.009},
  url          = {http://dx.doi.org/10.1016/j.ijar.2018.10.009},
  journal      = {International Journal of Approximate Reasoning},
  publisher    = {Elsevier BV}
}

@Article{oldArxiv,
  author       = {He, Jun and Wang, Liqun and Liu, Liu and Feng, Jiao
                  and Wu, Hao},
  title        = {Long Document Classification From Local Word
                  Glimpses via Recurrent Attention Learning},
  year         = 2019,
  volume       = 7,
  pages        = {40707–40718},
  issn         = {2169-3536},
  doi          = {10.1109/access.2019.2907992},
  url          = {http://dx.doi.org/10.1109/access.2019.2907992},
  journal      = {IEEE Access},
  publisher    = {Institute of Electrical and Electronics Engineers
                  (IEEE)}
}

@inproceedings{bigBird,
 author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and  Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Big Bird: Transformers for Longer Sequences},
 url = {https://papers.nips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf},
 year = {2020}
}

@Article{decisionThreshold,
  author       = {Chen, J. J. and Tsai, C.-A. and Moon, H. and Ahn,
                  H. and Young, J. J. and Chen, C.-H.},
  title        = {Decision threshold adjustment in class prediction},
  year         = 2006,
  volume       = 17,
  number       = 3,
  month        = {Jun},
  pages        = {337–352},
  issn         = {1029-046X},
  doi          = {10.1080/10659360600787700},
  url          = {http://dx.doi.org/10.1080/10659360600787700},
  journal      = {SAR and QSAR in Environmental Research},
  publisher    = {Informa UK Limited}
}


@TECHREPORT{multilabelReview2,
    author = {Mohammad S Sorower},
    title = {A literature survey on algorithms for multi-label learning},
    institution = {},
    year = {2010}
}

@ARTICLE{hammingLoss,
title = {Multi-Label Classification: An Overview},
author = {Tsoumakas, Grigorios and Katakis, Ioannis},
year = {2007},
journal = {International Journal of Data Warehousing and Mining (IJDWM)},
volume = {3},
number = {3},
pages = {1-13},
abstract = {Multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization, and semantic scene classification. This article introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multilabel classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.},
url = {https://EconPapers.repec.org/RePEc:igg:jdwm00:v:3:y:2007:i:3:p:1-13}
}

@article{multilabelMethods,
abstract = {Multi-label learning has received significant attention in the research community over the past few years: this has resulted in the development of a variety of multi-label learning methods. In this paper, we present an extensive experimental comparison of 12 multi-label learning methods using 16 evaluation measures over 11 benchmark datasets. We selected the competing methods based on their previous usage by the community, the representation of different groups of methods and the variety of basic underlying machine learning methods. Similarly, we selected the evaluation measures to be able to assess the behavior of the methods from a variety of view-points. In order to make conclusions independent from the application domain, we use 11 datasets from different domains. Furthermore, we compare the methods by their efficiency in terms of time needed to learn a classifier and time needed to produce a prediction for an unseen example. We analyze the results from the experiments using Friedman and Nemenyi tests for assessing the statistical significance of differences in performance. The results of the analysis show that for multi-label classification the best performing methods overall are random forests of predictive clustering trees (RF-PCT) and hierarchy of multi-label classifiers (HOMER), followed by binary relevance (BR) and classifier chains (CC). Furthermore, RF-PCT exhibited the best performance according to all measures for multi-label ranking. The recommendation from this study is that when new methods for multi-label learning are proposed, they should be compared to RF-PCT and HOMER using multiple evaluation measures.
► Experimental comparison of 12 multi-label learning methods on 11 benchmark datasets. ► Experimental evaluation using 16 performance measures and training and testing times. ► SVM-based methods perform better on small datasets compared to the other methods. ► RF-PCT and HOMER perform best over all evaluation measures.},
author = {Madjarov, Gjorgji and Kocev, Dragi and Gjorgjevikj, Dejan and Džeroski, Sašo},
address = {Kidlington},
copyright = {2012 Elsevier Ltd},
issn = {0031-3203},
journal = {Pattern recognition},
keywords = {Multi-label ranking ; Multi-label classification ; Comparison of multi-label learning methods ; Signal and communications theory ; Telecommunications and information theory ; Exact sciences and technology ; Signal, noise ; Applied sciences ; Information, signal and communications theory ; Signal representation. Spectral analysis},
language = {eng},
number = {9},
pages = {3084-3104},
publisher = {Elsevier Ltd},
title = {An extensive experimental comparison of methods for multi-label learning},
volume = {45},
year = {2012},
}

@Article{lossTopKError,
  author       = {Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
  title        = {Loss Functions for Top-k Error: Analysis and
                  Insights},
  year         = 2016,
  month        = {Jun},
  doi          = {10.1109/cvpr.2016.163},
  url          = {http://dx.doi.org/10.1109/cvpr.2016.163},
  isbn         = 9781467388511,
  journal      = {2016 IEEE Conference on Computer Vision and Pattern
                  Recognition (CVPR)},
  publisher    = {IEEE}
}

@inproceedings{topKmulticlassSVM,
author = {Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
title = {Top-k Multiclass SVM},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {325–333},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@Article{LTR,
  author       = {Liu, Tie-Yan},
  title        = {Learning to Rank for Information Retrieval},
  year         = 2011,
  doi          = {10.1007/978-3-642-14267-3},
  url          = {http://dx.doi.org/10.1007/978-3-642-14267-3},
  isbn         = 9783642142673,
  publisher    = {Springer Berlin Heidelberg}
}

@Article{SS,
  author       = {Doersch, Carl and Zisserman, Andrew},
  title        = {Multi-task Self-Supervised Visual Learning},
  year         = 2017,
  month        = {Oct},
  doi          = {10.1109/iccv.2017.226},
  url          = {http://dx.doi.org/10.1109/iccv.2017.226},
  isbn         = 9781538610329,
  journal      = {2017 IEEE International Conference on Computer
                  Vision (ICCV)},
  publisher    = {IEEE}
}

@Article{Rep,
  author       = {Bengio, Y. and Courville, A. and Vincent, P.},
  title        = {Representation Learning: A Review and New
                  Perspectives},
  year         = 2013,
  volume       = 35,
  number       = 8,
  month        = {Aug},
  pages        = {1798–1828},
  issn         = {2160-9292},
  doi          = {10.1109/tpami.2013.50},
  url          = {http://dx.doi.org/10.1109/tpami.2013.50},
  journal      = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  publisher    = {Institute of Electrical and Electronics Engineers
                  (IEEE)}
}

@INPROCEEDINGS{YOLO,
  author={J. {Redmon} and S. {Divvala} and R. {Girshick} and A. {Farhadi}},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={You Only Look Once: Unified, Real-Time Object Detection}, 
  year={2016},
  volume={},
  number={},
  pages={779-788},
  doi={10.1109/CVPR.2016.91}
}


@INPROCEEDINGS{FaceNet,
  author={F. {Schroff} and D. {Kalenichenko} and J. {Philbin}},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={FaceNet: A unified embedding for face recognition and clustering}, 
  year={2015},
  volume={},
  number={},
  pages={815-823},
  doi={10.1109/CVPR.2015.7298682}
}

@Article{ULMFit,
  author       = {Howard, Jeremy and Ruder, Sebastian},
  title        = {Universal Language Model Fine-tuning for Text
                  Classification},
  year         = 2018,
  doi          = {10.18653/v1/p18-1031},
  url          = {http://dx.doi.org/10.18653/v1/p18-1031},
  journal      = {Proceedings of the 56th Annual Meeting of the
                  Association for Computational Linguistics (Volume 1:
                  Long Papers)},
  publisher    = {Association for Computational Linguistics}
}

@article{distilBert,
  author    = {Victor Sanh and
               Lysandre Debut and
               Julien Chaumond and
               Thomas Wolf},
  title     = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper
               and lighter},
  journal   = {CoRR},
  volume    = {abs/1910.01108},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.01108},
  archivePrefix = {arXiv},
  eprint    = {1910.01108},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mobileNet,
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
year = {2018},
month = {06},
pages = {4510-4520},
title = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
doi = {10.1109/CVPR.2018.00474}
}

@inproceedings{imagenet,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"
}

@article{bigBSArxiv,
  author    = {Samuel L. Smith and
               Pieter{-}Jan Kindermans and
               Quoc V. Le},
  title     = {Don't Decay the Learning Rate, Increase the Batch Size},
  journal   = {CoRR},
  volume    = {abs/1711.00489},
  year      = 2017,
  url       = {http://arxiv.org/abs/1711.00489},
  archivePrefix = {arXiv},
  eprint    = {1711.00489},
  timestamp = {Mon, 13 Aug 2018 16:46:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-00489.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{XLNet,
 author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {5753--5763},
 publisher = {Curran Associates, Inc.},
 title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
 url = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
 volume = {32},
 year = {2019}
}

@Article{saturation,
  author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton,
                  Geoffrey E.},
  title        = {ImageNet classification with deep convolutional
                  neural networks},
  year         = 2017,
  volume       = 60,
  number       = 6,
  month        = {May},
  pages        = {84–90},
  issn         = {1557-7317},
  doi          = {10.1145/3065386},
  url          = {http://dx.doi.org/10.1145/3065386},
  journal      = {Communications of the ACM},
  publisher    = {Association for Computing Machinery (ACM)}
}

@Article{extremeSIGIR,
  author       = {Liu, Jingzhou and Chang, Wei-Cheng and Wu, Yuexin
                  and Yang, Yiming},
  title        = {Deep Learning for Extreme Multi-label Text
                  Classification},
  year         = 2017,
  month        = {Aug},
  doi          = {10.1145/3077136.3080834},
  url          = {http://dx.doi.org/10.1145/3077136.3080834},
  isbn         = 9781450350228,
  journal      = {Proceedings of the 40th International ACM SIGIR
                  Conference on Research and Development in
                  Information Retrieval},
  publisher    = {ACM}
}


@inproceedings{multilabelMetrics,
 author = {Koyejo, Oluwasanmi O and Natarajan, Nagarajan and Ravikumar, Pradeep K and Dhillon, Inderjit S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {3321--3329},
 publisher = {Curran Associates, Inc.},
 title = {Consistent Multilabel Classification},
 url = {https://proceedings.neurips.cc/paper/2015/file/85f007f8c50dd25f5a45fca73cad64bd-Paper.pdf},
 volume = {28},
 year = {2015}
}


@INPROCEEDINGS{weightedMetrics,
  author={B. {Behera} and G. {Kumaravelan} and P. {Kumar B.}},
  booktitle={2019 11th International Conference on Advanced Computing (ICoAC)}, 
  title={Performance Evaluation of Deep Learning Algorithms in Biomedical Document Classification}, 
  year={2019},
  volume={},
  number={},
  pages={220-224},
  doi={10.1109/ICoAC48765.2019.246843}
}

@inproceedings{queryClassification,
author = {Kang, In-Ho and Kim, GilChang},
title = {Query Type Classification for Web Document Retrieval},
year = {2003},
isbn = {1581136463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/860435.860449},
doi = {10.1145/860435.860449},
abstract = {The heterogeneous Web exacerbates IR problems and short user queries make them worse. The contents of web documents are not enough to find good answer documents. Link information and URL information compensates for the insufficiencies of content information. However, static combination of multiple evidences may lower the retrieval performance. We need different strategies to find target documents according to a query type. We can classify user queries as three categories, the topic relevance task, the homepage finding task, and the service finding task. In this paper, a user query classification scheme is proposed. This scheme uses the difference of distribution, mutual information, the usage rate as anchor texts, and the POS information for the classification. After we classified a user query, we apply different algorithms and information for the better results. For the topic relevance task, we emphasize the content information, on the other hand, for the homepage finding task, we emphasize the Link information and the URL information. We could get the best performance when our proposed classification method with the OKAPI scoring algorithm was used.},
booktitle = {Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval},
pages = {64–71},
numpages = {8},
keywords = {query classification, URL information, combination of multiple evidences, link information},
location = {Toronto, Canada},
series = {SIGIR '03}
}

@inproceedings{textCategorization,
author = {Bruno, Trstenjak and Sasa, Mikac and Donko, Dzenana},
year = {2013},
month = {11},
pages = {},
title = {KNN with TF-IDF based framework for text categorization},
volume = {69},
journal = {Procedia Engineering},
doi = {10.1016/j.proeng.2014.03.129}
}

@phdthesis{IRClassStat,
  title={Information retrieval using statistical classification},
  author={Hull, David A},
  year={1994},
  school={Citeseer}
}

@article{statTextCategorization,
  title={An Evaluation of Statistical Approaches to Text Categorization},
  author={Yiming Yang},
  journal={Information Retrieval},
  year={2004},
  volume={1},
  pages={69-90}
}

@inproceedings{imageClassification,
author = {Shen, Fumin and Mu, Yadong and Yang, Yang and Liu, Wei and Liu, Li and Song, Jingkuan and Shen, Heng Tao},
title = {Classification by Retrieval: Binarizing Data and Classifiers},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080767},
doi = {10.1145/3077136.3080767},
abstract = {This paper proposes a generic formulation that significantly expedites the training and deployment of image classification models, particularly under the scenarios of many image categories and high feature dimensions. As the core idea, our method represents both the images and learned classifiers using binary hash codes, which are simultaneously learned from the training data. Classifying an image thereby reduces to retrieving its nearest class codes in the Hamming space. Specifically, we formulate multiclass image classification as an optimization problem over binary variables. The optimization alternatingly proceeds over the binary classifiers and image hash codes. Profiting from the special property of binary codes, we show that the sub-problems can be efficiently solved through either a binary quadratic program (BQP) or a linear program. In particular, for attacking the BQP problem, we propose a novel bit-flipping procedure which enjoys high efficacy and a local optimality guarantee. Our formulation supports a large family of empirical loss functions and is, in specific, instantiated by exponential and linear losses. Comprehensive evaluations are conducted on several representative image benchmarks. The experiments consistently exhibit reduced computational and memory complexities of model training and deployment, without sacrificing classification accuracy.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {595–604},
numpages = {10},
keywords = {hashing, classification, binary codes},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{documentClassification,
author = {Blosseville, M. J. and H\'{e}brail, G. and Monteil, M. G. and P\'{e}not, N.},
title = {Automatic Document Classification: Natural Language Processing, Statistical Analysis, and Expert System Techniques Used Together},
year = {1992},
isbn = {0897915232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/133160.133175},
doi = {10.1145/133160.133175},
abstract = {In this paper we describe an automated method of classifying research project descriptions: a human expert classifies a sample set of projects into a set of disjoint and pre-defined classes, and then the computer learns from this sample how to classify new projects into these classes. Both textual and non-textual information associated with the projects are used in the learning and classification phases. Textual information is processed by two methods of analysis: a natural language analysis followed by a statistical analysis. Non-textual information is processed by a symbolic learning technique. We present the results of some experiments done on real data: two different classifications of our research projects.},
booktitle = {Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {51–58},
numpages = {8},
location = {Copenhagen, Denmark},
series = {SIGIR '92}
}

@inproceedings{Amoualian2020SIGIR2E,
  	title={SIGIR 2020 E-Commerce Workshop Data Challenge Overview},
  	author={Amoualian, Hesam and Goswami, Parantapa and Ach, Laurent and Das, Pradipto and Montalvo, Pablo},
  	year={2020},
	booktitle = {Proceedings Proceedings of ACM SIGIR Workshop on eCommerce (SIGIR eCom’20). ACM},
	publisher = {ACM},
	location = {New York, NY, USA},
	numpages = {6},
}

@INPROCEEDINGS{multilabelComparison,  author={G. {Nasierding} and A. Z. {Kouzani}},  booktitle={2012 9th International Conference on Fuzzy Systems and Knowledge Discovery},   title={Comparative evaluation of multi-label classification methods},   year={2012},  volume={},  number={},  pages={679-683},  doi={10.1109/FSKD.2012.6234347}}

@article{pairwiseBinary,
title = {Label ranking by learning pairwise preferences},
journal = {Artificial Intelligence},
volume = {172},
number = {16},
pages = {1897-1916},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020800101X},
author = {Eyke Hüllermeier and Johannes Fürnkranz and Weiwei Cheng and Klaus Brinker},
keywords = {Preference learning, Ranking, Pairwise classification, Constraint classification},
abstract = {Preference learning is an emerging topic that appears in different guises in the recent literature. This work focuses on a particular learning scenario called label ranking, where the problem is to learn a mapping from instances to rankings over a finite number of labels. Our approach for learning such a mapping, called ranking by pairwise comparison (RPC), first induces a binary preference relation from suitable training data using a natural extension of pairwise classification. A ranking is then derived from the preference relation thus obtained by means of a ranking procedure, whereby different ranking methods can be used for minimizing different loss functions. In particular, we show that a simple (weighted) voting strategy minimizes risk with respect to the well-known Spearman rank correlation. We compare RPC to existing label ranking methods, which are based on scoring individual labels instead of comparing pairs of labels. Both empirically and theoretically, it is shown that RPC is superior in terms of computational efficiency, and at least competitive in terms of accuracy.}
}

@INPROCEEDINGS{pairwiseNet,  author={E. {Loza Mencia} and J. {Furnkranz}},  booktitle={2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)},   title={Pairwise learning of multilabel classifications with perceptrons},   year={2008},  volume={},  number={},  pages={2899-2906},  doi={10.1109/IJCNN.2008.4634206}}
