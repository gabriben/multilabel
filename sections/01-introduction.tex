% !TEX root = ../main.tex

\section{introduction}
\label{sec:org662677c}


Many real-world classification problems are challenging because of unclear (or overlapping) class-boundaries, subjectivity issues, and disagreement between annotators.
For example, the movie \textit{Tenet} generated not just debates about its content, but also about which movie genre it belongs to. IMDB simultaneously categorizes it as \textit{action}, \textit{sci-fi}, and \textit{thriller}.\footnote{\url{https://www.imdb.com/title/tt6723592/}.}
As a second example, a seminal machine learning paper that proposes to train with bigger batch sizes~\citep{bigBSArxiv} is categorized on \textit{arXiv} as \textit{Machine Learning (cs.LG)},
\textit{Computer Vision and Pattern Recognition (cs.CV)}, \textit{Distributed,
Parallel, and Cluster Computing (cs.DC)}, and \textit{Machine Learning
(stat.ML)}.\footnote{\url{https://arxiv.org/abs/1711.00489}.}
These two examples are representative of a large class of multilabel classification problems.
%
\begin{comment}
\begin{enumerate}[label=(\arabic*),leftmargin=*]
\item The possibility of assigning more than one label to a single instance is desirable (i.e., labels are not mutually exclusive).
\item The instance being labeled needs to be inspected or consumed in its entirety before a full set of class labels can be determined. For example, it requires an entire viewing of the movie \textit{Tenet} to determine if the label \textit{romance} is appropriate, as it is arguably the underlying driver of the protagonists.
% Instead, it would be hard to isolate a simple characteristic of an instance that is uniquely predictive of a label.
\item The number of labels differs per instance, making the number of labels to assign at inference time unknown.
\end{enumerate}
\end{comment}
%
%\paragraph{Learning task}
In \ac{IR}, multilabel learning tasks are very common, e.g., document and text classification often deal with multilabel and multiclass (a subfield of multilabel classification where a single label is attributed to an example) problems~\cite{IRClassStat, textCategorization, statTextCategorization, documentClassification}, as do query classification~\cite{queryClassification, introIR}, image classification~\cite{imageClassification, faceDetection} and product classification~\cite{Amoualian2020SIGIR2E}. 


\if0
As we show in our experiments and the related work discussed below, multilabel learning tasks are very common in \ac{IR}. Document and text classification are often a \todo{FIMPUL} task and have historically focused a lot of attention in \ac{IR}~\cite{IRClassStat, textCategorization, statTextCategorization, documentClassification}. Other \ac{IR} related examples are query classification~\cite{queryClassification, introIR}, image classification~\cite{imageClassification, faceDetection} and most recently the \textit{multimodal product classification and retrieval challenge} at SIGIR 2020~\cite{Amoualian2020SIGIR2E}. 
\fi

%\paragraph{Previous solutions to multilabel tasks}
To solve multilabel learning tasks, existing optimization frameworks are typically based on variations of the cross-entropy, logistic, or sigmoid loss. \citeauthor{multilabelReduction} define these as \emph{multilabel reduction} techniques, with an emphasis on two: One-Versus-All (OVA) and Pick-All-Labels (PAL)~\cite{multilabelReduction}. OVA and PAL reformulate the multilabel problem respectively to $C$ binary classification and $C$ multiclass classification problems, with $C$ the number of possible classes for labels (see~\ref{section:background:estimate} for more details). These methods come with the assumption that marginal probabilities of the suitability of a label for an example (a.k.a. Bayes Optimal Classifier~\cite{OVA2}) are independent of other label propensities. Consequently, there is a gap between the approximated quantity and the real multilabel evaluation task. The other shortcoming shared by OVA and PAL is the lack of a holistic approach for both label count and label prediction.

Within the multilabel reduction framework, the particular settings of extreme multilabel classification is studied intensely. With thousands~\cite{extremeClassification} or even millions~\cite{millionsOfLabels, extremeMilliionsSlice} of labels, specific solutions are required, such as label embeddings~\cite{extremeMultilabelEmbeddings} or negative mining~\cite{stochasticNegativeMining}. Another prolific subfield of multilabel reduction, hierarchical labeling, allows one to constrain the algorithm to learn $K$ (1 or more) labels per group in the hierarchy. For example, DBPedia~\citep{lehmann2015dbpedia} establishes a hierarchical structure in Wikipedia infoboxes and is commonly used to finetune state-of-the-art NLP models~\citep[see, e.g.,][]{XLNet, ULMFit}. 

Parallel to the multilabel reduction approaches, recent advances in dealing with sparse class representation for multiclass classification~\citep{focalLoss,tencent}, seem promising for the multilabel setting. Also recently, a multi-task approach to predicting both label propensity and the number of labels was proposed to solve the multilabel prediction setting~\cite{multitaskLabel}.

The methods above either reformulate the problem (multilabel reduction) or deal with subfields\footnote{In most of this work, the term \emph{multilabel classification} excludes \emph{extreme}, \emph{hierarchical} or \emph{multiclass} subfields.} (extreme / hierarchical multilabel classification). While we found the multi-task approach~\cite{multitaskLabel} tailored to multilabel problems, to the best of our knowledge, there is no generic loss function to deal with multilabel classification in a modern deep learning setting in a single task.



\paragraph{Our proposed solution to multilabel tasks}
We propose an alternative type of loss function that (I) naturally approximates a multilabel metric (as shown in Table~\ref{table:overallresults}), (II) estimates label propensities and label counts (see Equation \ref{eq:unboundedF1}) (III) and is decomposable for stochastic gradient descent (per our definition of a smooth surrogate, see Section~\ref{subsection:properties} and Figure~\ref{fig:sigmoid}). Our proposed solution is to use a surrogate of the F1 metric as a loss. This solution does not reformulate our assumptions about multilabel data: it is the formulation of a loss function designed for the problem at hand, as opposed to using an existing optimization framework or optimizing over the sum of several loss functions.
Using a metric as a loss function is not uncommon, but is unpopular for metrics that require a form of thresholding (e.g., counting the number of true positives), as minimizing a step loss function is intractable~\cite{stochasticNegativeMining}. In the ranking domain, LambdaLoss has been proposed to optimize directly for the lambdaRank metric~\cite{lambdaLoss}. We follow a different strategy for the multilabel classification domain, where the step function is approximated by a sigmoid curve. 

\if0
\gab{alternative formulations for Our proposed solution to multilabel tasks:}
\begin{itemize}[leftmargin=*,nosep]
\item{What is the metric that is optimized for here? While it has been shown that some of these formulations are consistent (as defined by..) with the metrics precision\@K and recall\@K, there is little evidence that the underlying deterministic formulation of multilabel classification is approximated.}
\item{We know that we can formulate metrics that are tailored to the multilabel setting. Why not formulate tailored losses as well
We propose to ask the opposite question: what is the loss that optimizes for the metrics at hand?}
\item{But there is few literature that solve the issue of common multilabel problems (up to a few hundreds of labels) with custom losses. Instead variations of the cross-entropy or sigmoid loss (multiclass and binary classification losses) are used (a.k.a. multilabel reductions~\cite{multilabelReduction}).}
\end{itemize}
\daan{Seems like this will all go? I think the first point is a good one to make and the second point we can write into the story a bit more.}
\fi

\paragraph{Main contributions of the paper}
We introduce \emph{sigmoidF1}, an F1 score surrogate, with a sigmoid function acting as a surrogate thresholding step function.
\emph{sigmoidF1} allows for the use of the F1 metric that simultaneously optimizes for label prediction and label counts in a single task.
\emph{sigmoidF1} is benchmarked against loss functions commonly used in multilabel learning and other existing multilabel models. We show that our custom losses improve predictions over current solutions on several different metrics, across text and image classification tasks.

%\paragraph{The remainder of the paper}
\vspace{0.5\baselineskip}
The remainder of this paper is structured as follows. First, we introduce background material and cover related work. We then introduce our method and define a class of smooth loss functions for multilabel problems. Next, we detail our experimental setup and describe the datasets used in our experiments. After presenting the experimental results in the next section, we conclude the paper with conclusions and suggestions for future work.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: