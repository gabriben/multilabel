% !TEX root = ../main.tex

\section{introduction}
\label{sec:org662677c}


Many real-world classification problems are challenging because of unclear (or overlapping) class-boundaries, subjectivity issues, and disagreement between annotators.
For example, the movie \textit{Tenet} generated not just debates about its content, but also on which movie genre it belongs to. IMDB simultaneously categorizes it as \textit{action}, \textit{sci-fi}, and \textit{thriller}.\footnote{See \url{https://www.imdb.com/title/tt6723592/}.}
As a second example, a seminal machine learning paper that proposes to train with bigger batch sizes~\citep{bigBSArxiv} is categorized on \textit{arXiv} as \textit{Machine Learning (cs.LG)},
\textit{Computer Vision and Pattern Recognition (cs.CV)}, \textit{Distributed,
Parallel, and Cluster Computing (cs.DC)}, and \textit{Machine Learning
(stat.ML)}.\footnote{See \url{https://arxiv.org/abs/1711.00489}.}
These two examples are representative of a large class of multilabel classification problems.

\begin{comment}
\begin{enumerate}[label=(\arabic*),leftmargin=*]
\item The possibility of assigning more than one label to a single instance is desirable (i.e., labels are not mutually exclusive).
\item The instance being labeled needs to be inspected or consumed in its entirety before a full set of class labels can be determined. For example, it requires an entire viewing of the movie \textit{Tenet} to determine if the label \textit{romance} is appropriate, as it is arguably the underlying driver of the protagonists.
% Instead, it would be hard to isolate a simple characteristic of an instance that is uniquely predictive of a label.
\item The number of labels differs per instance, making the number of labels to assign at inference time unknown.
\end{enumerate}
\end{comment}

\paragraph{Learning task}
In \ac{IR}, multilabel learning tasks are very common, e.g., document and text classification often deal with multiclass and multilabel problems \ac{IR}~\cite{IRClassStat, textCategorization, statTextCategorization, documentClassification}, as do query classification~\cite{queryClassification, introIR}, image classification~\cite{imageClassification, faceDetection} and product classification~\cite{Amoualian2020SIGIR2E}. 

\if0
As we show in our experiments and the related work discussed below, multilabel learning tasks are very common in \ac{IR}. Document and text classification are often a \todo{FIMPUL} task and have historically focused a lot of attention in \ac{IR}~\cite{IRClassStat, textCategorization, statTextCategorization, documentClassification}. Other \ac{IR} related examples are query classification~\cite{queryClassification, introIR}, image classification~\cite{imageClassification, faceDetection} and most recently the \textit{multimodal product classification and retrieval challenge} at SIGIR 2020~\cite{Amoualian2020SIGIR2E}. 
\fi

\paragraph{Previous solutions to multilabel tasks}
To solve multilabel learning tasks, existing optimization frameworks are typically based on variations of the cross-entropy, logistic, or sigmoid loss. \citeauthor{multilabelReduction} define these as \emph{multilabel reduction techniques}: One-Versus-All (OVA), Pick-All-Labels (PAL) and Pick-One-Label (POL)~\cite{multilabelReduction}. These methods come with the assumption that marginal probabilities of the suitability of a label for an example (a.k.a. Bayes Optimal Classifier) are independent of other label propensities. 
\mdr{The other type of approach seems to have vanished.}
An important shortcoming shared by both\daan{huh? OVA, PAL \& POL?} classes of solutions is the lack of a holistic approach for both label count and label prediction and the distance from the real evaluation task.
\daan{I think we need to explain these three techniques a bit more here. Maybe include a few word explanation of each in your own words here?}

Recent advances \mdr{in what} deal with specific aspects of multiclass classification, such as sparsity~\citep{focalLoss,tencent}. For the multilabel setting, the particular setting of extreme multilabel classification is studied intensenly. That setting with tens of thousands~\cite{extremeClassification} or even millions~\cite{millionsOfLabels, extremeMilliionsSlice} of labels requires specific solutions, such as label embeddings~\cite{extremeMultilabelEmbeddings} or negative mining~\cite{stochasticNegativeMining}. We do not study extreme multilabel classification, but seek to adapt existing classification algorithms for multiclass multilabel classification problems, for which we could find little existing literature~\citep{multilabelMethods}.
\gab{I think this distinction is important, seeing how most papers cited are extreme labeling papers in \cite{multilabelReduction}}


\paragraph{Our proposed solution to multilabel tasks}
We propose an alternative type of loss function that is aimed at balancing prediction of label propensities and label count prediction, while modeling interactions between label propensities. Our proposed solution is to use the F1 metric as a loss. This solution does not reformulate our assumptions about data: it is the formulation of a loss function designed for the problem at hand, as opposed to using an existing optimization framework or optimizing over the sum of several loss functions.
Using a metric as a loss function is not uncommon, but is unpopular for metrics that require a form of thresholding (e.g., counting the number of true positives), as minimizing a step loss function is intractable~\cite{stochasticNegativeMining}. In the ranking domain, LambdaLoss has been proposed to optimize directly for the lambdaRank metric~\cite{lambdaLoss}. We follow a similar strategy for the multilabel classification domain. 
%\gab{name-dropping lambdaloss because it is a CIKM paper. Not sure it is the best place.}\daan{Kinda ok here, but the "analog endeavor" makes our efforts seem smaller.}

\gab{alternative formulations for Our proposed solution to multilabel tasks:}
\begin{itemize}[leftmargin=*,nosep]
\item{What is the metric that is optimized for here? While it has been shown that some of these formulations are consistent (as defined by..) with the metrics precision\@K and recall\@K, there is little evidence that the underlying deterministic formulation of multilabel classification is approximated.}
\item{We know that we can formulate metrics that are tailored to the multilabel setting. Why not formulate tailored losses as well
We propose to ask the opposite question: what is the loss that optimizes for the metrics at hand?}
\item{But there is few literature that solve the issue of common multilabel problems (up to a few hundreds of labels) with custom losses. Instead variations of the cross-entropy or sigmoid loss (multiclass and binary classification losses) are used (a.k.a. multilabel reductions~\cite{multilabelReduction}).}
\end{itemize}
\daan{Seems like this will all go? I think the first point is a good one to make and the second point we can write into the story a bit more.}


\paragraph{Main contributions of the paper}
We propose a general mathematical formulation of multilabel tasks.
Our formulation encompasses different levels of complexity, from the classical cross-entropy loss up to the proposed loss function. We introduce \emph{sigmoidF1}, an F1 score surrogate, with a sigmoid function acting as a surrogate thresholding step function.
\emph{sigmoidF1} allows for the use of the F1 metric that simultaneously optimizes for label prediction and label counts in a single task.
\emph{sigmoidF1} is benchmarked against loss functions commonly used in multilabel learning and other existing multilabel models. We show that our custom losses improve predictions over the current state-of-the-art on several different metrics, across text and image classification tasks.

\paragraph{The remainder of the paper}
The remainder of this paper is structured as follows. First, we introduce background material and cover related work. We then introduce our method and define a class of smooth loss functions for multilabel problems. Next, we detail our experimental setup and describe the datasets used in our experiments. After presenting the experimental results in the next section, we conclude the paper with conclusions and suggestions for future work.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: