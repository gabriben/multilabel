% !TEX root = ../main.tex

\section{introduction}
\label{sec:org662677c}


Many real-world classification problems are challenging because of unclear (or overlapping) class-boundaries, subjectivity issues and disagreement between annotators.
For example, the movie \textit{Tenet} generated not just debates about its content, but also on which movie genre it belongs to. IMDB simultaneously categorizes it as \textit{action}, \textit{sci-fi}, and \textit{thriller}.\footnote{See \url{https://www.imdb.com/title/tt6723592/}.}
As a second example, a seminal machine learning publication that proposes to train with bigger batch sizes~\citep{bigBSArxiv} is categorized on \textit{arXiv} as \textit{Machine Learning (cs.LG)},
\textit{Computer Vision and Pattern Recognition (cs.CV)}, \textit{Distributed,
Parallel, and Cluster Computing (cs.DC)}, and \textit{Machine Learning
(stat.ML)}.\footnote{See \url{https://arxiv.org/abs/1711.00489}.}
These two examples are representative of a large class of multilabel classification problems.


\begin{comment}
\begin{enumerate}[label=(\arabic*),leftmargin=*]
\item The possibility of assigning more than one label to a single instance is desirable (i.e., labels are not mutually exclusive).
\item The instance being labeled needs to be inspected or consumed in its entirety before a full set of class labels can be determined. For example, it requires an entire viewing of the movie \textit{Tenet} to determine if the label \textit{romance} is appropriate, as it is arguably the underlying driver of the protagonists.
% Instead, it would be hard to isolate a simple characteristic of an instance that is uniquely predictive of a label.
\item The number of labels differs per instance, making the number of labels to assign at inference time unknown.
\end{enumerate}
\end{comment}

\paragraph{Learning task}

% multilabel classification is ... ~\citep{hammingLoss} ~\citep{multilabelReview} 

% We refer to learning tasks that share the three characteristics listed above as Full-instance, Multilabel Prediction for Unknown Label counts, or FIMPUL, for short.
% Characteristic (1) is captured by the \emph{multilabel} predicate; characteristic (2) by \emph{full-instance}, and characteristic (3) by \emph{unknown label count}.
As we show in our experiments and the related work discussed below, multilabel learning tasks are very common in \ac{IR}. Document and text classification are often a FIMPUL task and have historically focused a lot of attention in \ac{IR}~\cite{IRClassStat, textCategorization, statTextCategorization, documentClassification}. Other \ac{IR} related examples are query classification~\cite{queryClassification, introIR}, image classification~\cite{imageClassification, faceDetection} and most recently the \textit{multimodal product classification and retrieval challenge} at SIGIR 2020~\cite{Amoualian2020SIGIR2E}. 

\paragraph{Previous solutions to multilabel tasks}
To solve multilabel learning tasks, existing optimization frameworks are typically based on variations of the cross-entropy, logistic or sigmoid loss. Menon et. al. define these as multilabel reduction techniques: One-Versus-All (OVA), Pick-All-Labels (PAL) and Pick-One-Label (POL)~\cite{multilabelReduction}. These methods come with the assumption that marginal probabilities of the suitability of a label for an example (a.k.a. Bayes Optimal Classifier) are independent of other label propensities. An important shortcoming shared by both classes of solutions is the lack of a holistic approach for both label count and label prediction and the distance from the real evaluation task.

There are recent advances dealing with specific aspects of multiclass classification such as sparsity~\citep[see, e.g.,][]{focalLoss,tencent}. Regarding the specific multilabel setting, modern solutions often veer towards the extreme multilabel case. That setting with tens of thousands~\cite{extremeClassification} or even millions~\cite{millionsOfLabels, extremeMilliionsSlice} of labels requires solutions of their own, such as label embeddings~\cite{extremeMultilabelEmbeddings} or negative mining~\cite{stochasticNegativeMining}. Importantly, we do not tackle extreme multilabel classification. \gab{I think this distinction is important, seeing how most papers cited are extreme labeling papers in \cite{multilabelReduction}}

On the other hand, we could find little literature on endeavors to adapt existing classification algorithms for the problem at hand~\citep{multilabelMethods}.

\paragraph{Our proposed solution to multilabel tasks}
We propose an alternative type of loss function that is aimed at balancing prediction of label propensities and label count prediction, while modeling interactions between label propensities. Our proposed solution, F1 metric as a loss, does not reformulate our assumptions about data: it is the formulation of a loss function designed for the problem at hand, as opposed to using one existing optimization framework or optimizing over the sum of several loss functions.
Using a metric as a loss function is not uncommon, but is unpopular for metrics that require a form of thresholding (e.g. counting the number of true positives): minimizing a step loss function is intractable~\cite{stochasticNegativeMining}. In the ranking domain, lambdaloss has been proposed to optimize directly for the lambdaRank metric~\cite{lambdaLoss}. We propose the analog endeavor for the multilabel classification domain. \gab{name-dropping lambdaloss because it is a CIKM paper. Not sure it is the best place.}

\gab{alternative formulations for Our proposed solution to multilabel tasks:}
\begin{itemize}
\item{What is the metric that is optimized for here? While it has been shown that some of these formulations are consistent (as defined by..) with the metrics precision\@K and recall\@K, there is little evidence that the underlying deterministic formulation of multilabel classification is approximated.}
\item{We know that we can formulate metrics that are tailored to the multilabel setting. Why not formulate tailored losses as well
We propose to ask the opposite question: what is the loss that optimizes for the metrics at hand?}
\item{But there is few literature that solve the issue of common multilabel problems (up to a few hundreds of labels) with custom losses. Instead variations of the cross-entropy or sigmoid loss (multiclass and binary classification losses) are used (a.k.a. multilabel reductions~\cite{multilabelReduction}).}
\end{itemize}


\paragraph{Main contributions of the paper}
We propose a general mathematical formulation of multilabel tasks.
Our formulation encompasses different levels of complexity, from the classical cross-entropy loss up to the proposed loss function. We introduce \emph{sigmoidF1}, an F1 score surrogate, with a sigmoid function acting as a surrogate thresholding step function.
\emph{sigmoidF1} allows for the use of the F1 metric that simultaneously optimizes for label prediction and label counts in a single task.
\emph{sigmoidF1} is benchmarked against loss functions commonly used in multilabel learning and other existing multilabel models. We show that our custom losses improve predictions over the current state-of-the-art on several different metrics, across text and image classification tasks.

\paragraph{The remainder of the paper}
The remainder of this paper is structured as follows: first, we introduce our method and define a class of smooth loss functions for multilabel problems. Next, we detail our experimental setup and describe the datasets used in our experiments. After presenting the experimental results in the next section, we close the paper with conclusions and suggestions for future work.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: