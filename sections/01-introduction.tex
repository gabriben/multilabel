% !TEX root = ../main.tex

\section{introduction}
\label{sec:org662677c}


Many real-world classification problems are challenging because of unclear (or overlapping) class-boundaries, subjectivity issues and disagreement between annotators.
For example, the movie \textit{Tenet} generated not just debates about its content, but also on which movie genre it belongs to. IMDB simultaneously categorizes it as \textit{action}, \textit{sci-fi}, and \textit{thriller}.\footnote{See \url{https://www.imdb.com/title/tt6723592/}.}
As a second example, a seminal machine learning publication that proposes to train with bigger batch sizes~\citep{bigBSArxiv} is categorized on \textit{arXiv} as \textit{Machine Learning (cs.LG)},
\textit{Computer Vision and Pattern Recognition (cs.CV)}, \textit{Distributed,
Parallel, and Cluster Computing (cs.DC)}, and \textit{Machine Learning
(stat.ML)}.\footnote{See \url{https://arxiv.org/abs/1711.00489}.}
These two examples are representative of a large class of classification problems that share several characteristics:

\begin{comment}
\begin{enumerate}[label=(\arabic*),leftmargin=*]
\item The possibility of assigning more than one label to a single instance is desirable (i.e., labels are not mutually exclusive).
\item The instance being labeled needs to be inspected or consumed in its entirety before a full set of class labels can be determined. For example, it requires an entire viewing of the movie \textit{Tenet} to determine if the label \textit{romance} is appropriate, as it is arguably the underlying driver of the protagonists.
% Instead, it would be hard to isolate a simple characteristic of an instance that is uniquely predictive of a label.
\item The number of labels differs per instance, making the number of labels to assign at inference time unknown.
\end{enumerate}
\end{comment}

\paragraph{Learning task}

multilabel classification is ... ~\citep{hammingLoss} ~\citep{multilabelReview} 

We refer to learning tasks that share the three characteristics listed above as Full-instance, Multilabel Prediction for Unknown Label counts, or FIMPUL, for short.
Characteristic (1) is captured by the \emph{multilabel} predicate; characteristic (2) by \emph{full-instance}, and characteristic (3) by \emph{unknown label count}.
As we show in our experiments and the related work discussed below, FIMPUL learning tasks are very common in \ac{IR}. Document and text classification are often a FIMPUL task and have historically focused a lot of attention in \ac{IR}~\cite{IRClassStat, textCategorization, statTextCategorization, documentClassification}. Other \ac{IR} related examples are query classification~\cite{queryClassification, introIR}, image classification~\cite{imageClassification, faceDetection} and most recently the \textit{multimodal product classification and retrieval challenge} at SIGIR 2020~\cite{Amoualian2020SIGIR2E}.

[Agrawal et al., 2013, Yu et al., 2014, Bhatia et al., 2015, Jain et al., 2016, Babbar and Sch√∂lkopf, 2017, Yen et al., 2017, Prabhu et al., 2018, Jain et al., 2019, Reddi et al., 2019].

\paragraph{Previous solutions to FIMPUL tasks}
To solve multilabel learning tasks, existing optimization frameworks are typically based on variations of the cross-entropy loss, with some recent advances dealing with specific aspects such as sparsity~\citep[see, e.g.,][]{focalLoss,tencent}.
Existing algorithmic solutions to deal with FIMPUL tasks can be divided into \emph{fit-data-to-algorithm} solutions, which map FIMPUL problems to a known problem formulation like multiclass uni-label classification, and \emph{fit-algorithm-to-data} solutions, which adapt existing classification algorithms for the problem at hand~\citep{multilabelMethods}. In the fit-data-to-algorithm solutions, cross-entropy losses are used at training time and thresholding is done at inference time to determine how many labels should be assigned to an instance. In the fit-algorithm-to-data solutions, elements of the learning algorithm are changed (such as the back propagation procedure or the tasks). An important shortcoming shared by both classes of solutions is the lack of a holistic approach for both label count and label prediction.

\paragraph{Our proposed solution to FIMPUL tasks}
We propose an alternative type of solution that is aimed at balancing prediction of label propensities and label count prediction. Our proposed solution, Confusion Matrix Metrics as Losses (\solution), is neither of the fit-algorithm-to-data type nor of the fit-data-to-algorithm type.
Instead, it is best characterized as an example of a \emph{design-algorithm-for-data} approach: the formulation of a loss function designed for the problem at hand, as opposed to using one existing optimization framework or optimizing over the sum of several loss functions.

\paragraph{Main contributions of the paper}
We propose a general mathematical formulation of FIMPUL tasks.
Our formulation encompasses different levels of complexity, from the classical cross-entropy loss up to the proposed loss function. We propose \solution. As a specific instance of the \solution, we introduce \emph{sigmoidF1}, an F1 score surrogate, with a sigmoid function acting as a surrogate thresholding step function.
\emph{sigmoidF1} allows for the use of the F1 metric that simultaneously optimizes for label prediction and label counts in a single task.
\emph{sigmoidF1} is benchmarked against loss functions commonly used in multilabel learning and other existing models that are closely related to the FIMPUL setting. We show that our custom losses improve predictions over the current state-of-the-art on several different metrics, across text and image classification tasks.

\paragraph{The remainder of the paper}
The remainder of this paper is structured as follows: first, we introduce our method and define a class of smooth loss functions for FIMPUL problems. Next, we detail our experimental setup and describe the datasets used in our experiments. After presenting the experimental results in the next section, we close the paper with conclusions and suggestions for future work.

bayes optimal classifier: true probability

using the same loss and metric: we are not the first ones to do that (lambdaloss) and it is not a problem theoretically.

what is the metric that cross-entropy / softmax directly optimize for? what is the loss that optimizes for the metrics?

The google Neurips paper formulates how different multilabel reductions 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: