% !TEX root = ../main.tex

\section{Background}
\label{section:background}

To build up to our approach, we use a traditional statistical framework as a guideline for multilabel classificaton methods~\citep{tukey}. We distinguish the desired theoretical statistic (the \textbf{estimand}), its functional form (the \textbf{estimator}) and its approximation (the \textbf{estimate}); the latter estimates can be benchmarked with \textbf{metrics}. We show how multilabel reductions tend to reformulate the estimand and treat labels independently (i.e. change our assumptions about the data). However, with a proper estimator, it is possible to directly model the estimand. Our proposed loss function, \textbf{sigmoidF1}, accommodates for the true estimand.

We define a learning algorithm $\mathcal{F}$ (i.e. a class of estimators) that maps inputs to outputs given a set of hyperparameters \(\mathcal{F}(\cdot ; \Theta): \mathcal{X} \rightarrow \mathcal{Y}\). We consider a particular case, with the input vector \(\mathbf{x} = \{x_1, \ldots, x_n\}\) and each observation is assigned $k$ labels (one\daan{zero?} or more) \(\mathbf{l} = \{l_1, \ldots, l_C\}\) out of a set of $C$ classes. \(y_{i}^{j}\) are binary variables, indicating presence of a label for each observation \(i\) and class \(j\). Together they form the matrix output $Y$.

\subsection{Estimand and definition of the risk}
\label{section:background:estimand}

We distinguish between two scenarios: the \emph{multiclass} and the \emph{multilabel} scenario. 
In the multiclass scenario, a single example is attributed one class label (e.g., classification of an animal on a picture). 
In the multilabel scenario, a single example can be assigned more than one class label (e.g., movie genres). 
%Multiclass classification deserves a mention towards the end, as it illustrates a straightforward relationship between estimand-estimator-estimate. 
We focus on the multilabel scenario. More formally, for a particular input $\mathbf{x}$ (e.g. paper abstracts) and output $Y$ (e.g. scientific field) its risk formulation can be written as in ~\citep{multilabelReduction}:
%
\begin{equation}
R_{\mathrm{ML}}(\mathcal{F}) = \mathbb{E}_{(\mathbf{x}, Y)}\left[\mathcal{L}_{\mathrm{ML}}(Y, \mathcal{F}(\mathbf{x}))\right].
\end{equation}
%
We define $\mathcal{F}$ as the estimand: the theoretical statistic. For one item $x_i$, the theoretical risk defines how close the estimand can get to that deterministic output vector $\mathbf{y}_{i}$.

\if0
For example, corn ($x_{corn}$) is eaten by a finite number of animals that can digest it ($\mathbf{y}_{corn} = {y_{corn}^{horse}, \ldots, y_{corn}^{sheep}} $), assuming animals belong to a finite set of living beings. If one were to predict which food is eaten by which animals, the theoretical risk (estimand) defines how close we can get to that deterministic vector $\mathbf{y}_{corn}$, given $x_{corn}$.
\fi

In practice, statistical models do output probabilities $\mathbf{\hat{y}}_{i}$ via an estimator and its estimate (also called propensities or suitabilities~\citep{multilabelReduction}). The solution to that stochastic-deterministic incompatibility is either to convert the estimator to a deterministic measure via decision thresholds (e.g. traditional cross-entropy loss), or to reformulate the estimand as a stochastic measure (our sigmoidF1 loss proposal). \daan{So we also reformulate the problem? I think this explanation is clear, so just to be sure of the storyline.}

\subsection{Estimator: the functional form}
\label{section:background:estimator}

The estimator $f \in \mathcal{F}$ is any minimizer of the risk $R_{ML}$. Predicting multiple labels per example comes with the assumption that labels are non-mutually exclusive.

\begin{proposition}
  The multilabel estimator of $y_{i}^{j}$ is dependent on the input and other labels for that example,
%
\begin{equation}
  \hat{y}_i^j = f(x, y_{i}^{1}, \ldots, y_{i}^{j-1}) = P(y_i^j = 1 | x, y_{i}^{1}, \ldots, y_{i}^{j-1})
\end{equation}
\label{eq:estimand}
\end{proposition}
%
 By proposing this general formulation, we entrench that characteristic in the estimator. Contrary to \citet{multilabelReduction}, we propose an estimator that models interdependence between labels and deals with thresholding for the estimate at training time for free (see next section).


\subsection{Estimate: approximation via a loss function}
\label{section:background:estimate}

Most of the literature found on multilabel classification can be characterized as multilabel reductions~\cite{multilabelReduction}. Given the general non-convex optimization context, the surrogate loss function $\mathcal{L}(\mathbf{y}_i, \hat{\mathbf{y}}_i)$ can take different forms. 

\subsubsection*{One-versus-all (OVA)}
We write\daan{textual glue needed here. Also: they write, not we, right?}
%
\begin{equation}
\begin{aligned}
\mathcal{L}_{\mathrm{OVA}}(y, f) &= \sum_{i \in[L]} \mathcal{L}_{\mathrm{BC}}\left(y_{i}, f_{i}\right)\\
&=\sum_{i \in[L]}\left\{y_{i} \cdot \mathcal{L}_{\mathrm{BC}}\left(1, f_{i}\right)+\left(1-y_{i}\right) \cdot \mathcal{L}_{\mathrm{BC}}\left(0, f_{i}\right)\right\},
\end{aligned}
\end{equation}
%
where $\mathcal{L}_{\mathrm{BC}}$ is some binary classification loss (a.k.a.\ binary relevance \cite{OVA1, hammingLoss, OVA2}), most often logistic loss.  Note that minimizing binary cross-entropy is equivalent to maximizing for log-likelihood~\cite[Section 4.3.4]{Bishop}.

\subsubsection*{Pick-all-labels (PAL)}
The loss function we set here is
%
\begin{equation}
\mathcal{L}_{\mathrm{PAL}}(y, f) = \sum_{i \in[L]} y_{i} \cdot \mathcal{L}_{\mathrm{MC}}(i, f).
\end{equation}
%
\mdr{Not all of the notation is explained.}
In this formulation, each example $(x_i, y_i)$ is converted to a multiclass framework, with one observation per positive label. The sum of inherently multiclass losses is used to represent the multilabel estimand, using for example softmax cross entropy. Note that cross-entropy loss can be formulated as \(\mathcal{L}_{\text {CE}}=-\sum \log \left(p_{i}\right)\). \daan{$p_i$ is not explained.}

\subsubsection*{Pick-one-label (POL)}
\daan{drop POL?}
For this reduction, given an example $(x,y)$, a single random positive label is chosen as the true label of $x$

\vspace{\baselineskip}

Note that OVA and PAL have each a form normalised by the number of positive labels~\cite{multilabelReduction}.

Multilabel reduction methods are characterized by their way of reformulating the estimand, the resulting estimator and the estimate. In particular, this allows the use of existing losses: logistic loss (for binary classification formulations), sigmoid loss or softmax cross-entropy loss (for multiclass formulations). These reductions imply a reformulation of the estimator (a.k.a. Bayes Optimal) as follows:
%
\begin{equation}
  y_i = \mathcal{F}(x) = P(y_i = 1 | x)
\end{equation}
%
Contrary to our definition of the original multilabel estimand (Eq.~\ref{eq:estimand}), independence of labels propensities is assumed. \gab{the reformulation of the estimator also implies a reformulation of the estimand like so...}. In other words the loss function becomes any monotone transformation of the marginal label probabilities $ P(y_i = 1 | x)$ ~\cite{OVA2, multilabelMetrics, unifiedView}

OVA and PAL have been shown to optimize for either precision or recall (see next section and the definition of consistency)~\cite{multilabelReduction}.

\daan{next two paragraphs does not help anymore. Either integrate in story or remove.}
Multilabel reductions has previously been referred to as \emph{fit-data-to-algorithm} solutions (a.k.a. \emph{problem tranformation}) as opposed to \emph{fit-algorithm-to-data} solutions (a.k.a.\ \emph{algorithm adaptation})~\cite{multilabelReview, multilabelReview2}. In Section~\ref{section:background:fitdata} we propose a fit-algorithm-to-data solution (i.e. a loss function), where the estimand and assumptions about the data are not reformulated.

Note that for many real world problems and datasets reducing the problem to top-k selection or establishing a hierarchical structure is an oversimplification, especially when classes are not mutually exclusive. Besides the four datasets we use for our experiments, we mention others in the related work section.

\subsection{Metrics: evaluation at inference time}
\label{section:background:metrics}

There is a consensus around the use of confusion matrix metrics (CMM) to evaluate multilabel classification models (at inference time) \gab{sources here}. Notably Precision and Recall. CMMs come with caveats: most of these measures 
\begin{enumerate*}
\item require a hard thresholding, and that makes them non-differentiable for Stochastic Gadiant Descent, 
\item they are very sensitive to the choice of the number top labels to include $k$\footnote{In the case of unilabel prediction, top-k becomes a top-1 problem, which essentially eliminates caveats I and II.} and 
\item they require aggregation choices to be made in terms of micro / macro / weighted metrics.
\end{enumerate*}
Some common CMMs are Precision, Recall, F1-score, hinge-loss or one-error-loss. Numerous others can be found on the confusion matrix Wikipedia page.
\mdr{Wikipedia is not a credible source of formal definitions in a CS paper.}

\textbf{consistency}

Similarly to \citet{multilabelReduction} and in the lineage of~\citep{consistency-surrogates, consistency-multiclassSVM, consistency-lossAnalysis}, we can define consistency as
%
\begin{equation}
\operatorname{reg}\left(f ; \mathcal{L}_{\mathrm{ML}}\right) \rightarrow 0 \Longrightarrow \operatorname{reg}\left(f ; \mathcal{L}\right) \rightarrow 0
\end{equation}
%
with $reg(f)$ the regret of an estimator with respect to its loss $l_{MC}$ is $\operatorname{reg}\left(f ; \mathcal{L}_{\mathrm{ML}}\right) = R_{\mathrm{ML}}(f)-\inf _{g: x \rightarrow \mathbb{R}^{L}} R_{\mathrm{ML}}(g)$

a loss is defined as consistent if it approximates the original metric.

\citet{multilabelReduction} show that OVA, PAL, POL and their normalized counterparts are consistent with either precision or recall. 

Note that loss functions, such as sigmoid loss or softmax cross-entropy loss are already suited to the multiclass purpose. The softmax cross-entropy and other losses have been shown to be consistent \cite{consistency-multiclassSVM, consistency-lossAnalysis, consistency-surrogates}.



\subsection{Multilabel blend estimate: F1 Metric as a Loss}
\label{section:background:metricsAsLosses}

To begin, we recall the definitions of precision, recall and F1 score:
%
\begin{equation}
	\begin{aligned} 
		\text { Precision } &=\frac{t p}{t p+f p} \\ 
		\text{ Recall } &=\frac{t p}{t p+f n} 
	\end{aligned}
\end{equation}
%
With F1 score the harmonic mean of precision and recall:
%
\begin{equation}
F_1=2 \cdot \frac{\text { precision } \cdot \text { recall }}{\text { precision }+\text { recall }}
\end{equation}
%
\mdr{What is: }This is our contribution. Similar to lambdaLoss and others... It implicitly deals with label counts and label predictions. It is also consistent and balances precision and recall \gab{TODO: show that}
%
\begin{equation}
\operatorname{reg}\left(f ; \mathcal{L}_{\mathrm{F_1}}\right) \rightarrow 0 \Longrightarrow \operatorname{reg}\left(f ; \mathrm{F_1}\right) \rightarrow 0
\end{equation}
%
We propose a generic loss which does not require hard thresholding decisions, , 

In a number of retrieval tasks, a model's out of sample accuracy is measured
on metrics such as AUROC, F1 score, etc. These reflect an objective catered
towards evaluating the model over an entire ranking. Due to the lack of
differentiability, these metrics cannot be directly used as loss functions at
training time (in-sample). A seminal study~\cite{optimizableLosses} derived a
general framework for deriving decomposable surrogates to some of these
metrics. We propose our own decomposable surrogates tailored for the problem
at hand.

In a typical machine learning classification tasks, binary labels are compared to a probabilistic measure (or a reversible
transformation of a probabilistic measure such as a sigmoid or a softmax
function). If the number $n_i$ of labels to be predicted per
example is known a priori, it is natural at training time to assign the $top_{n_i}$ predictions
to that example~\cite{lossTopKError, topKmulticlassSVM}. If the number of
labels per example is not known a priori, the question remains at both training and at inference time
as to how to decide on the number of labels to assign to each
example. This is generally done via a \emph{decision threshold}, that can be set globally for all
examples. This threshold can optimize for specificity or
sensitivity~\cite{decisionThreshold}. We propose an approach where this threshold
is implicitly defined, by using a loss function that penalizes explicitly for wrong label counts.

\cite{multiclassToBinary1}


% \begin{figure}[t]
% \centering
% \includegraphics[width=.9\linewidth]{./tree/Tree.pdf}
% \caption{\label{fig:tree} SIMPUL (bold) within the \emph{multiclass}
% nomenclature
% \hvk{figure is not referenced in text, bold is unclear in figure}
% \daan{I think it should go. And if it stays: uni -> single.}
% % Clarifying ``multiclass'' classification problems. In this paper we focus on
% % the uni-instance, multilabel, multiclass classification problem with a
% % varying number of labels (the bottom right hand side of the tree).
% }
% \end{figure}% \mdr{Image source ...}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: