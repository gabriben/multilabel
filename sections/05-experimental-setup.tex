% !TEX root = ../main.tex

\section{Experimental Setup}
\label{sec:orgb44ba25}

sigmoidF1 is tested across different modalities, namely image, video, sound and text, with a focus on text: the most comparable research was on text data.

% \doubt{optional paragraph}
% In light of the problem definition leading to the sigmoidF1 framework in the introduction and in order to clearly delimit the proposed method, following are a few datasets that are not suitable for the task.


Among the three datasets used for benchmarking ML-NET \cite{multitaskLabel}, a cancer hallmark dataset is of multi-instance multilabel nature \cite{cancerHallmarks}\footnote{Available at \url{https://www.cl.cam.ac.uk/&sim;sb895/HoC.html}}: the research clearly describe a process of annotating several expressions within paper abstracts. The remaining two datasets for chemical exposure \cite{chemExposure}\footnote{Available at \url{https://figshare.com/articles/Corpus_and_Software/4668229}} and diagnosis codes assigment \cite{diagnosisCode}, \footnote{Available at \url{https://physionet.org/works/ICD9CodingofDischargeSummaries}} seem to fit to the entity wide multilabel definition but have a strong hierarchical nature. Although slightly out-of-scope, the three datasets above will be used for benchmarking, since they were used to test ML-NET, which is the state-of-the-art in \emph{algorithm adaptation} for text to the best of our knowledge.

For a broader scope in learning for text data, we also use the newly created \emph{Arxiv dataset}\footnote{Available at \url{https://www.kaggle.com/Cornell-University/arxiv}} with data on abstracts of 1.7 million open source articles and their categories (suitably mutually inclusive and of varying count per example). This dataset is to be distinguished from an earlier long document Arxiv datset: it consisted of content scraped from a small number of PDFs with 11 classes \cite{oldArxiv}. This dataset was used by \cite{}

In the vision domain, a dataset of movie posters\footnote{Labels available at https://tinyurl.com/y7ydyedu and prescraped images from IMDB at https://tinyurl.com/y7lfpvlx} and their genre is used. Similarly, labels are mutually inclusive and of varying count per example. It is arguable that is hard to single out elements in the image of a poster that define the genre of a movie. Rather it might be a combination of the title font, the background image, the presence of actors and specific objects such as cars, weapons etc. 


\textbf{hardware}
Cloud computing was used to perform experiments. Databricks was used to train models on AWS machines with 1, 8, or 16 GPUs in parallel, depending on the needs (\textit{g4dn.12xlarge}, \textit{p2.8xlarge}, \textit{p2.16xlarge}).


\textbf{setup}
We set an irrelevance threshold $t$ on both datasets. If a class appears less than $t$ times in the dataset, it is considered irrelevant. For the Arxiv2020 Dataset, we set it at 1000, which results in 145 classes. For moviePosters we set it at 1000 as well, which results in 14 classes.

\todo{I removed all jpg's that are empty in the prescraped data. I could try to scrape the posters myself to see if I get more}

Another recently created dataset was made available for \emph{Large Scale Holistic Video Understanding} \cite{holisticVideoData} \footnote{Available at https://github.com/holistic-video-understanding/HVU-Dataset}, as defined in the introduction.

% Cancer can be described according to its complexity with different principles, named hallmarks cite:cancerHallmarks. A corpus of 1580 PubMed abstracts are manually annotated for 10 hallmarks. This is a multi-instance labelling task and will therefore not be used here.

% [[./images/cancerHallmarksAnnotation.jpg]]

% - Multilabel classification for text cite:toxicComments

% - Scenery dataset for images cite:dataScenery.

\todo{read http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf and http://adas.cvc.uab.es/task-cv2016/papers/0002.pdf}

\todo{this is an ambitious number of datasets. Add longer description of each dataset, depending on which ones I keep: sample size, number of classes etc. see utils here: https://github.com/ashrefm/multi-label-soft-f1}

\doubt{cite Kaggle datasets formally instead of using links: https://www.kaggle.com/data/46091}

\doubt{add a music genre classification dataset, for which Vincent Koops at RTL could help train}


