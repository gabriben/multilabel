% !TEX root = ../main.tex

\section{Experimental Setup}
\label{sec:orgb44ba25}

sigmoidF1 is tested across different modalities, namely image, video, sound and text, with a focus on text: the most comparable research was on text data.

% \doubt{optional paragraph}
% In light of the problem definition leading to the sigmoidF1 framework in the introduction and in order to clearly delimit the proposed method, following are a few datasets that are not suitable for the task.

\subsection{data}

\todo{see if related work section comes before this and introduces ML-NET}

For a broad scope in learning for text, we use the newly created \emph{Arxiv dataset}\footnote{Available at \url{https://www.kaggle.com/Cornell-University/arxiv}} with data on abstracts of 1.7 million open source articles and their categories (suitably mutually inclusive and of varying count per example). This dataset is to be distinguished from an earlier long document Arxiv datset: it consisted of content scraped from a small number of PDFs with 11 classes \cite{oldArxiv} and was used in a recent long transformer publication~\cite{bigBird}. In \textit{Arxiv2020}, we subset the \emph{Arxiv dataset} into its 2020 content, given limited computing power. This results in around 26k abstracts.


ML-NET~\cite{multitaskLabel} is the state-of-the-art in \emph{algorithm adaptation} for text to the best of our knowledge. Among the three datasets used for benchmarking ML-NET, the cancerHallmark~\cite{cancerHallmarks}\footnote{Available at \url{https://github.com/sb895/Hallmarks-of-Cancer}} and chemicalExposure~\cite{chemExposure}\footnote{Available at \url{https://github.com/sb895/chemical-exposure-information-corpus}} datasets are of multi-instance multilabel nature: several expressions are annotated within each paper abstracts. The third dataset diagnosisCodes could not be recovered, upon contacting the authors of both ML-NET and the original paper~\cite{diagnosisCode}. 

In the vision domain, a dataset of movie posters\footnote{Labels available at https://tinyurl.com/y7ydyedu and prescraped images from IMDB at https://tinyurl.com/y7lfpvlx} and their genre (e.g. \emph{action}, \emph{comedy}) is used. Similarly, labels are mutually inclusive and of varying count per example. 


It is arguable that is hard to single out elements in the image of a poster that define the genre of a movie. Rather it might be a combination of the title font, the background image, the presence of actors and specific objects such as cars, weapons etc.

\begin{table}
\caption{Datasets}
\label{tab:arxiv2020}
\centering
\begin{tabular}{l ccccc}
\toprule
Dataset & Type & \# classes & $\varnothing$ \# labels & sample size\\
\midrule
Arxiv2020 & text & 155 & 1.888 (2.0) & 26558\\ 
moviePosters & image & 0 & 0 & 0\\
cancerHallmarks & text & 302 & 7.317 (6) & 1582\\
chemExposure & text & 0 & 0 & 0\\
\bottomrule
\end{tabular}
\end{table}


\subsection{hardware}

We performed our experiments on AWS machines with parallelization on up to 16 GPUs \textit{p2.16xlarge}.

\subsection{Neural Network Architecture}

For the text datasets, Huggingface's pretrained DistilBert is used as it is the most efficient (accuracy/computing time) Bert model to date. For the image dataset, 

We set an irrelevance threshold $t$ on both datasets prior to conducting experiments, so as to not finetune for that feature. The sensitivity study below shows what happens when that constraint is relaxed. If a class appears less than $t$ times in the dataset, it is considered irrelevant. For the Arxiv2020 Dataset, we set it at 1000, which results in 145 classes. For moviePosters we set it at 1000 as well, which results in 14 classes.

Batch size is set at a high value of 256. We thus increase accuracy over existing losses~\cite{bigBS}, but also allow variety of examples within the batch, thus collecting enough values in the confusion matrix. In a future research, it might be interesting to establish the minimum required batch size for sigmoidF1.

In order to make sure that the results of different loss functions are comparable, we fix the model weights of the pretrained CNN and DistilBert and keep the hyperparameter values that were used to train them from scratch. We also use the same random seeds for the neural network and make sure to split the data in the same train/validation/test sets for each different loss function. you can download our code to retrieve the exact train, valid and test sets.

\textbf{how to fine tune for $\beta$ and $\eta$}
grud search, we found that it is enough to sample within this space. see plot.

% Cancer can be described according to its complexity with different principles, named hallmarks cite:cancerHallmarks. A corpus of 1580 PubMed abstracts are manually annotated for 10 hallmarks. This is a multi-instance labelling task and will therefore not be used here.

% [[./images/cancerHallmarksAnnotation.jpg]]

% - Multilabel classification for text cite:toxicComments

% - Scenery dataset for images cite:dataScenery.

\todo{read http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf and http://adas.cvc.uab.es/task-cv2016/papers/0002.pdf}

\todo{this is an ambitious number of datasets. Add longer description of each dataset, depending on which ones I keep: sample size, number of classes etc. see utils here: https://github.com/ashrefm/multi-label-soft-f1}

you can find the tencent loss here: https://github.com/Tencent/tencent-ml-images/blob/master/train.py



%%% Local Variables:
%%% mode: plain-tex
%%% TeX-master: "../meta"
%%% End:
