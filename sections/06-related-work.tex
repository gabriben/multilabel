% !TEX root = ../main.tex

\section{Related Work}
\label{sec:org2aceb9f}

% Beyond identifying object types (see YOLO~\cite{YOLO} and its successors),
% performing face recognition (see FaceNet\cite{FaceNet} and its successors) on
% segments of an image, n
% Neural networks are increasingly becoming better at predicting more abstract
% concepts via deeper networks, representation learning and
% self-supervision~\citep[see, e.g.,][]{SS,Rep}. There is a significant volume
% of recent work on building neural networks with a high-level of abstract
% understanding in the embedding space~\mdr{REF}. However, research on
% developing optimization frameworks that are adapted for these abstract
% concepts in the output space is limited. In the next sections we detail
% some of this related work and discuss it relation to the research in this paper.

Existing algorithmic solutions to deal with multilabel tasks can be divided into \emph{fit-data-to-algorithm} solutions, which map FIMPUL problems to a known problem formulation like multiclass uni-label classification, and \emph{fit-algorithm-to-data} solutions, which adapt existing classification
algorithms to the problem at hand~\citep{multilabelMethods}.

\subsection{Fit-data-to-algorithm}
Commonly, in fit-data-to-algorithm solutions, cross-entropy losses are used at training time and thresholding is done at inference time to determine how many labels should be assigned to an instance. This has also been called multilabel reduction ~\citep{multilabelReduction}, to be distinguished from multiclass to binary classifications ~\citep{multiclassToBinary1, multiclassToBinary2, multiclassToBinary3}.

We can further make the distinction between One-versus-all (OVA) and Pick-all-labels (PAL) solutions ~\citep{multilabelReduction} (see also~\ref{section:background}).

In OVA (a.k.a binary relevance model), one reduces the classification problem to independent binary classifications~\citep{OVA1, hammingLoss, OVA2}. OVA Theory \cite{OVATheory}.

In PAL , one ... \citep{labelPowerset, extremeClassification, PAL}. The \textit{label powerset} approach considers each unique set of labels as one class in the transformed setting~\cite{labelPowerset}.

In Pick-One-Label (POL), a single multiclass example is created by randomly sampling a positive label \cite{PAL, extremeClassification}.

Note that Menon et. al. propose a normalized versions of OVA and PAL~\cite{multilabelReduction}.

% (e.g., an instance labeled \textit{Thriller} and \textit{Action}, results in the creation of the class \textit{Thriller and action}).
Alternatively, \textit{ranking by pairwise comparison} is a solution where the dataset is duplicated for each possible label pairs. Each duplicated dataset has therefore two classes and only contains instances that have at least one of the labels in the label pair. Different ranking methods exist~\cite{pairwiseBinary, pairwiseNet}.

More recently, hierarchical datasets such as DBpedia~\citep{lehmann2015dbpedia} are used to finetune BERT-based models~\cite{XLNet, bigBird};  the latter publications use cross-entropy to predict the labels.

\subsection{Fit-algorithm-to-data}
In the fit-algorithm-to-data solutions, elements of the learning
algorithm are changed (such as the back propagation procedure or the task).
Early representatives of fit-algorithm-to-data stem from heterogenous domains
of machine learning. MultiLabel k-Nearest Neighbors \cite{ML-KNN},
MultiLabel Decision Tree \cite{ML-DT}, Ranking Support Vector Machine
\cite{multilabelSVM} and Backpropagation for MultiLabel Learning
\cite{multilabelBackprop}. More recently, two papers introduced the idea of
multi-task learning for \emph{label prediction} and \emph{label count
prediction} for text data \cite[ML\(_{\text{NET}}\)][]{multitaskLabel} and image
data \cite{multitaskLabelImages, tencent}. The latter research is loosely
catered towards object detection and is thus out-of-scope: local elements in a picture are predicted that tend to be uni-label as defined by the ground-truth (e.g., cat, flower, vase, person, bottle
etc.).

An important limitation shared by both \emph{fit-data-to-algorithm} and \emph{fit-algorithm-to-data} is the lack of a holistic approach for both label count and label prediction.

\begin{figure*}[t!]
\centering
\includegraphics[width=.9\linewidth]{./images/betaEtaResized.png}
\vspace{1\baselineskip}
\caption{\label{fig:betaEta}
DistilBert (NLP) + classification head on arXiv2020 â€“ different scores at a 0.5 threshold for different values of $\eta$ and $\beta$ in a sampling region similar to Figure~\ref{fig:sigmoid}}
\end{figure*}

\subsection{Thresholding}
\label{subsec:thresh}

Machine learning prediction tasks' output are probabilistic (or a reversible transformation of a probabilistic measure such as a sigmoid or a softmax function).
At training time, these outputs are compared to binary
values in the case of binary encoding of classes.

At inference time, if the number $n_i$ of labels to be predicted per example is known a priori, it is natural to assign the $top_{n_i}$ predictions to that example~\cite{lossTopKError, topKmulticlassSVM}.
If the number of labels per example is unknown a priori,  at inference time the question remains as to how to extract information about the number of labels to assign to each example, aside from the propensity of labels to be assigned.
This is generally done via a \emph{decision threshold}, that can be set globally for all examples, and be optimized for specificity or sensitivity~\cite{decisionThreshold}.

Thresholding across classes or examples can be an issue when the number of labels to predict is unknown. Some variants of cross-entropy loss accommodate imbalanced label data~\cite{focalLoss}, but remain agnostic to the number of labels to predict.
Solutions include determining an ideal global \emph{threshold} depending on the use-case at hand~\cite{threshForF1}, or per-class-thresholding after training~\cite{moviePosters} and eventually abstracting the threshold away via a \emph{soft-F1} measure~\cite{softF1}.
%In the latter two cases, the task is to predict genre from movie posters.

In our method, this threshold is defined implicitly, thanks to the use of metrics that already penalize for
wrong label counts.


\subsection{Metrics as losses}

In a number of learning to rank tasks~\cite{LTR}, a model's out of sample accuracy is measured on metrics such as AUROC, F1 score, etc. These reflect an objective catered towards evaluating the model over an entire ranking. Due to the lack of differentiability, these metrics cannot be directly used as loss
functions at training time (in-sample). \citet{optimizableLosses} propose a general framework for deriving
decomposable surrogates from some of these metrics.
Recently, a similar work has been proposed to train a Convolutional
Neural Network (CNN) from scratch with a few millions of images and hundreds
of labels specifically for multilabel tasks \cite{tencent}; this task is loosely related to object detection, similar to \cite{multitaskLabelImages}.
In our research, we instead propose decomposable surrogates of the classical confusion matrix metrics and in particular \emph{sigmoidF1}, tailored to the problem at hand.

% The proposed method is positioned in the lineage of \emph{algorithm
% adaptation}, using \emph{metric as losses} and allowing for dynamic
% \emph{thresholding}.

% This section will be guided by the previous section's formulation of the multitags problem, we will therefore focus on \emph{fit-algorithm-to-data}, \emph{metrics as losses} and \emph{thresholding}.

% \subsection{fit-algorithm-to-data}
% \label{sec:org150a474}

% % Early representatives of \emph{fit-algorithm-to-data} stem from heterogenous domains of machine learning. Multi-Label k-Nearest Neighbors \cite{ML-KNN}, Multi-Label Decision Tree \cite{ML-DT}, Ranking Support Vector Machine \cite{multilabelSVM} and Backpropagation for Multi-Label Learning \cite{multilabelBackprop}. More recently, two papers introduced the idea of multitask learning for \emph{label prediction} and \emph{label count prediction} for text (ML\(_{\text{NET}}\)) \cite{multitaskLabel} and image \cite{multitaskLabelImages} data. The latter research is loosely catered towards object detection (although not formally presented as such) and is thus out-of-scope: elements in a picture are predicted that tend to be unilabel as defined by the groundtruth (e.g. cat, flower, vase, person, bottle etc.).

% \subsection{Metrics as losses}
% \label{sec:orgb0a9d21}

% Often, machine learning post-training evaluation metrics (e.g. AUROC, F1) are not differentiable. There are motivations \todo{which motivations} for optimizing a model directly on a metric at training time. A general framework for AUC, AUROC and F1 is presented in \cite{optimizableLosses}, but the proposed F1 surrogate remains short of being explicitly derived for stochastic gradient descent. \todo{check again with the authors if I can't get inspired from their work}. Recently, a similar work has been proposed to train a Convolutional Neural Network (CNN) from scratch with a few millions of images and hundreds of labels specifically for multilabel tasks \cite{tencent}. This task is loosely related to object detection, similarly to \cite{multitaskLabelImages} mentioned in the previous paragraph.


% in reformulating loss functions to accomodate sparsity in the data, to optimize directly for the metric at hand or to do thresholding posthoc (see movie posters).

% \subsection{Thresholding}
% \label{sec:org8295f09}

% \emph{thresholding} accross classes or examples can be an issue as soon as the number of labels to predict is unknown. Certain variants of cross-entropy loss accommodate imbalanced label data  \cite{focalLoss}, but remain agnostic towards the number of labels to predict. Solutions have been tailored to that end, starting with determining an ideal global \emph{threshold} depending on use-cases \cite{threshForF1}, or per-class-thresholding after training \cite{moviePosters} and eventually abstracting the threshold away via a \emph{soft-F1} measure \cite{softF1} \todo{say more about this method}. In the latter two cases, the task is to predict genre from movie posters.



% The proposed method is positioned in the lineage of \emph{fit-algorithm-to-data}, using \emph{metric as losses} and allowing for dynamic \emph{thresholding}.

% \todo{compare to this:} \cite{lossComp}

% \todo{Hamming Loss}
% % \todo{Precision@K, Recall@K, NDCG@K}
% \todo{MLTSVM loss and the three-way loss inspired by it} \cite{MLTSVM} and \cite{MLTSVMThreeway}

% We propose a dynamic thresholding mechanism auto-tuned at training time.


% ** weak labels
% (unsure the labels are correct)

% - https://people.cs.pitt.edu/~kovashka/ye_zhang_kovashka_iccv2019_cap2det.pdf


% ** implementations

% *** movies

%  [[https://www.analyticsvidhya.com/blog/2019/04/build-first-multi-label-image-classification-model-python/][movie posters with classes]].

%  They have movie titles in them

% *** pretrained resnet on multilabel

%  https://github.com/Tencent/tencent-ml-images

% What happens when using a Resnet pretrained on multilabels

% *** soft F1 score loss

%  https://github.com/ashrefm/multi-label-soft-f1

% https://www.analyticsvidhya.com/blog/2019/04/build-first-multi-label-image-classification-model-python/



% /Optimizing directly for macro F1: By introducing the macro soft-F1 loss, we could train the model to directly increase the metric we care about: the macro F1-score @ threshold 0.5. We could clearly observe the alignment during training and evaluation on successive epochs. When using this loss, we do not have to tune the decision threshold any more. Imagine a multi-label classification system with hundreds of labels, how unstable the system will be if we have to continuously update the optimal threshold for each label. The macro soft-F1 loss comes to the rescue. By using it, we can keep all thresholds fixed at 0.5 and still get an optimal performance from the training process./



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
